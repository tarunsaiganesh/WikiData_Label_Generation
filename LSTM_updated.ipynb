{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"LSTM_updated.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8fjuCUzLaGl","executionInfo":{"status":"ok","timestamp":1623724254568,"user_tz":420,"elapsed":209,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"3b0c6e08-b8fd-4c05-cd93-51a7ff2a8c44"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Th8uphZpLZME","executionInfo":{"status":"ok","timestamp":1623724256534,"user_tz":420,"elapsed":141,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["import numpy as np\n","import torch"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y6pkuSsFLZMJ"},"source":["# CASE STUDY: POS Tagging!\n","\n","Now let's dive into an example that is more relevant to NLP and is relevant to your HW3, part-of-speech tagging! We will be building up code up until the point where you will be able to process the POS data into tensors, then train a simple model on it.\n","The code we are building up to forms the basis of the code in the homework assignment.\n","\n","To start, we'll need some data to train and evaluate on. First download the train and dev POS data `twitter_train.pos` and `twitter_dev.pos` into the same directory as this notebook."]},{"cell_type":"markdown","metadata":{"id":"R09CwR9GLZMM"},"source":["We will now be introducing three new components which are vital to training (NLP) models:\n","1. a `Vocabulary` object which converts from tokens/labels to integers. This part should also be able to handle padding so that batches can be easily created.\n","2. a `Dataset` object which takes in the data file and produces data tensors\n","3. a `DataLoader` object which takes data tensors from `Dataset` and batches them"]},{"cell_type":"markdown","metadata":{"id":"KXfkPQlALZMN"},"source":["### `Vocabulary`"]},{"cell_type":"markdown","metadata":{"id":"oaUSLgvdLZMO"},"source":["Next, we need to get our data into Python and in a form that is usable by PyTorch. For text data this typically entails building a `Vocabulary`  of all of the words, then mapping words to integers corresponding to their place in the sorted vocabulary. This can be done as follows:"]},{"cell_type":"code","metadata":{"code_folding":[6,23,28,31,37,41,49],"id":"AAW6G_BZLZMP","executionInfo":{"status":"ok","timestamp":1623724259169,"user_tz":420,"elapsed":155,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["class Vocabulary():\n","    \"\"\" Object holding vocabulary and mappings\n","    Args:\n","        word_list: ``list`` A list of words. Words assumed to be unique.\n","        add_unk_token: ``bool` Whether to create an token for unknown tokens.\n","    \"\"\"\n","    def __init__(self, word_list, add_unk_token=False):\n","        # create special tokens for padding and unknown words\n","        self.pad_token = '<pad>'\n","        self.unk_token = '<unk>' if add_unk_token else None\n","\n","        self.special_tokens = [self.pad_token]\n","        if self.unk_token:\n","            self.special_tokens += [self.unk_token]\n","\n","        self.word_list = word_list\n","        \n","        # maps from the token ID to the token\n","        self.id_to_token = self.word_list + self.special_tokens\n","        # maps from the token to its token ID\n","        self.token_to_id = {token: id for id, token in\n","                            enumerate(self.id_to_token)}\n","        \n","    def __len__(self):\n","        \"\"\" Returns size of vocabulary \"\"\"\n","        return len(self.token_to_id)\n","    \n","    @property\n","    def pad_token_id(self):\n","        return self.map_token_to_id(self.pad_token)\n","        \n","    def map_token_to_id(self, token: str):\n","        \"\"\" Maps a single token to its token ID \"\"\"\n","        return self.token_to_id[token]\n","\n","    def map_id_to_token(self, id: int):\n","        \"\"\" Maps a single token ID to its token \"\"\"\n","        return self.id_to_token[id]\n","\n","    def map_tokens_to_ids(self, tokens: list, max_length: int = None):\n","        \"\"\" Maps a list of tokens to a list of token IDs \"\"\"\n","        # truncate extra tokens and pad to `max_length`\n","        if max_length:\n","            tokens = tokens[:max_length]\n","            tokens = tokens + [self.pad_token]*(max_length-len(tokens))\n","        return [self.map_token_to_id(token) for token in tokens]\n","\n","    def map_ids_to_tokens(self, ids: list, filter_padding=True):\n","        \"\"\" Maps a list of token IDs to a list of token \"\"\"\n","        tokens = [self.map_id_to_token(id) for id in ids]\n","        if filter_padding:\n","            tokens = [t for t in tokens if t != self.pad_token]\n","        return tokens"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wspYT10iLZMR"},"source":["### `Dataset`"]},{"cell_type":"markdown","metadata":{"id":"Al2NWJhXLZMS"},"source":["Next, we need a way to efficiently read in the data file and to process it into tensors. PyTorch provides an easy way to do this using the `torch.utils.data.Dataset` class. We will be creating our own class which inherits from this class. \n","\n","Helpful link: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"SA4-gF0dLZMT"},"source":["A custom `Dataset` class must implement three functions: \n","\n","- $__init__$: The init functions is run once when instantisting the `Dataset` object.\n","- $__len__$: The len function returns the number of data points in our dataset.\n","- $__getitem__$. The getitem function returns a sample from the dataset give the index of the sample. The output of this part should be a dictionary of (mostly) PyTorch tensors."]},{"cell_type":"code","metadata":{"code_folding":[1,22,25,36,45,50,55],"id":"f1DxFQayLZMV","executionInfo":{"status":"ok","timestamp":1623726384721,"user_tz":420,"elapsed":159,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["class WikiDataset(torch.utils.data.Dataset):\n","    def __init__(self, data_path, testData: int):\n","        self._dataset = []\n","        self.isTestData = testData\n","        # read the dataset file, extracting tokens and tags\n","        with open(data_path, 'r', encoding='utf-8') as f:\n","            for i,line in enumerate(f):\n","                if(i==0): continue\n","                if(self.isTestData == 1 and i > 250): continue\n","                if(self.isTestData == 0 and i <= 250): continue\n","                sentence_tokens = []\n","                elements = line.strip().split(',')\n","                #sentence\n","                sentence = elements[3].split(' ')\n","                for word in sentence:\n","                    clean_word = word.replace(\".\", \"\").replace(\"(\",\"\").replace(\")\",\"\")\n","                    sentence_tokens.append(clean_word)\n","                #Origin Page\n","                origin_page = elements[0]\n","                #Destination Page\n","                destination_page = elements[1]\n","                #WikiData Labels\n","                labels = elements[4] if self.isTestData!=2 else None\n","                self._dataset.append({'sentence_tokens': sentence_tokens, 'origin_page': [origin_page], 'destination_page': [destination_page],'label': [labels]})\n","        \n","        # intiailize an empty vocabulary\n","        self.sentence_token_vocab = None\n","        self.origin_page_vocab = None\n","        self.destination_page_vocab = None\n","        self.label_vocab = None\n","\n","    def __len__(self):\n","        return len(self._dataset)\n","\n","    def __getitem__(self, item: int):\n","        # get the sample corresponding to the index\n","        instance = self._dataset[item]\n","        \n","        # check the vocabulary has been set\n","        assert self.sentence_token_vocab is not None\n","        assert self.origin_page_vocab is not None\n","        assert self.destination_page_vocab is not None\n","        assert self.label_vocab is not None\n","        \n","        # Convert inputs to tensors, then return\n","        return self.tensorize(instance['sentence_tokens'],instance['origin_page'], instance['destination_page'],instance['label'])\n","    \n","    def tensorize(self, sentence_tokens, origin_page, destination_page, cls, max_length=None):\n","        # map the sentence tokens into their ID form\n","        sentence_token_ids = self.sentence_token_vocab.map_tokens_to_ids(sentence_tokens, max_length)\n","        tensor_dict = {'sentence_token_ids': torch.LongTensor(sentence_token_ids)}\n","        # map origin_page\n","        origin_page_map = self.origin_page_vocab.map_tokens_to_ids(origin_page)\n","        tensor_dict['origin_page_id'] = torch.LongTensor(origin_page_map)\n","        # map destination_page\n","        destination_page_map = self.destination_page_vocab.map_tokens_to_ids(destination_page)\n","        tensor_dict['destination_page_id'] = torch.LongTensor(destination_page_map)\n","        # map class\n","        if self.isTestData != 2:\n","            label_map = self.label_vocab.map_tokens_to_ids(cls)\n","            tensor_dict['label'] = torch.LongTensor(label_map)\n","        return tensor_dict\n","        \n","    def get_sentence_tokens_list(self):\n","        s_tokens = [token for d in self._dataset for token in d['sentence_tokens']]\n","        return sorted(set(s_tokens))\n","    \n","    def get_origin_pages_list(self):\n","        o_pages = [op for d in self._dataset for op in d['origin_page']]\n","        return sorted(set(o_pages))\n","    \n","    def get_destination_pages_list(self):\n","        d_pages = [dp for d in self._dataset for dp in d['destination_page']]\n","        return sorted(set(d_pages))\n","\n","    def get_classes_list(self):\n","        assert self.isTestData != 2\n","        clss = [c for d in self._dataset for c in d['label']]\n","        return sorted(set(clss))\n","\n","    def set_vocab(self, sentence_token_vocab: Vocabulary, origin_page_vocab: Vocabulary, destination_page_vocab: Vocabulary, label_vocab: Vocabulary):\n","        self.sentence_token_vocab = sentence_token_vocab\n","        self.origin_page_vocab = origin_page_vocab\n","        self.destination_page_vocab = destination_page_vocab\n","        self.label_vocab = label_vocab"],"execution_count":82,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1g7dNRjELZMX"},"source":["Now let's create `Dataset` objects for our training and validation sets!\n","A key step here is creating the `Vocabulary` for these datasets.\n","We will use the list of words in the training set to intialize a `Vocabulary` object over the input words. \n","We will also use list of tags to intialize a `Vocabulary` over the tags."]},{"cell_type":"code","metadata":{"id":"hR_zMHHXLZMY","executionInfo":{"status":"ok","timestamp":1623727923893,"user_tz":420,"elapsed":1228,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["train_dataset = WikiDataset('/content/drive/Shared drives/CS272/data/extended_datasets/roman_history_training_dedup.csv', testData = 0)\n","valdn_dataset = WikiDataset('/content/drive/Shared drives/CS272/data/extended_datasets/roman_history_training_dedup.csv', testData = 1)\n","test_dataset = WikiDataset('/content/drive/Shared drives/CS272/data/extended_datasets/roman_history_training_dedup.csv', testData = 2)\n","\n","# Get list of sentence tokens, origin pages, destination pages, and classes in train set\n","train_sentence_token_list = train_dataset.get_sentence_tokens_list()\n","train_origin_page_list = train_dataset.get_origin_pages_list()\n","train_destination_page_list = train_dataset.get_destination_pages_list()\n","train_class_list = train_dataset.get_classes_list()\n","\n","# Get list of sentence tokens, origin pages, destination pages, and classes in valdn set\n","valdn_sentence_token_list = valdn_dataset.get_sentence_tokens_list()\n","valdn_origin_page_list = valdn_dataset.get_origin_pages_list()\n","valdn_destination_page_list = valdn_dataset.get_destination_pages_list()\n","valdn_class_list = valdn_dataset.get_classes_list()\n","\n","# Get list of sentence tokens, origin pages, destination pages in test set\n","test_sentence_token_list = test_dataset.get_sentence_tokens_list()\n","test_origin_page_list = test_dataset.get_origin_pages_list()\n","test_destination_page_list = test_dataset.get_destination_pages_list()\n","\n","#Create Vocabulary\n","sentence_token_vocab = Vocabulary(sorted(set(train_sentence_token_list + valdn_sentence_token_list + test_sentence_token_list)), add_unk_token=False)\n","origin_page_vocab = Vocabulary(sorted(set(train_origin_page_list + valdn_origin_page_list + test_origin_page_list)), add_unk_token=False)\n","destination_page_vocab = Vocabulary(sorted(set(train_destination_page_list + valdn_destination_page_list + test_destination_page_list)), add_unk_token=False)\n","label_vocab = Vocabulary(sorted(set(train_class_list + valdn_class_list)), add_unk_token=False)\n","\n","# Update the train/valdn/test set with vocabulary.\n","train_dataset.set_vocab(sentence_token_vocab, origin_page_vocab, destination_page_vocab, label_vocab)\n","valdn_dataset.set_vocab(sentence_token_vocab, origin_page_vocab, destination_page_vocab, label_vocab)\n","test_dataset.set_vocab(sentence_token_vocab, origin_page_vocab, destination_page_vocab, label_vocab)"],"execution_count":112,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBFzYB1hLZMZ","executionInfo":{"status":"ok","timestamp":1623727927142,"user_tz":420,"elapsed":162,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"5d77bbb0-2843-47de-9e8c-e07bf14f88a9"},"source":["print(f'Size of training set: {len(train_dataset)}')\n","print(f'Size of validation set: {len(valdn_dataset)}')\n","print(f'Size of test set: {len(test_dataset)}')\n","print(f'sentence_token_vocab: {len(sentence_token_vocab)}')\n","print(f'origin_page_vocab: {len(origin_page_vocab)}')\n","print(f'destination_page_vocab: {len(destination_page_vocab)}')"],"execution_count":113,"outputs":[{"output_type":"stream","text":["Size of training set: 12099\n","Size of validation set: 250\n","Size of test set: 12349\n","sentence_token_vocab: 27054\n","origin_page_vocab: 5676\n","destination_page_vocab: 1880\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1rLzEH7SLZMb"},"source":["Let's print out one data point of the tensorized data and see what it looks like"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Umfyv0HkLZMd","executionInfo":{"status":"ok","timestamp":1623727473644,"user_tz":420,"elapsed":233,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"d29cec76-941e-4687-9d70-53791c22f655"},"source":["instance = train_dataset[0]\n","print(instance)\n","\n","sentence_tokens = train_dataset.sentence_token_vocab.map_ids_to_tokens(instance['sentence_token_ids'])\n","origin_page = train_dataset.origin_page_vocab.map_ids_to_tokens(instance['origin_page_id'])\n","destination_page = train_dataset.destination_page_vocab.map_ids_to_tokens(instance['destination_page_id'])\n","cls = train_dataset.label_vocab.map_ids_to_tokens(instance['label'])\n","print()\n","print(f'Tokens: {sentence_tokens}')\n","print(f'OriginPage:   {origin_page}')\n","print(f'DestinationPage:   {destination_page}')\n","print(f'Class:   {cls}')"],"execution_count":105,"outputs":[{"output_type":"stream","text":["{'sentence_token_ids': tensor([ 6385, 13964, 14205,  4638, 14581, 17537, 15517, 13147, 15587,  3405,\n","        12189, 17205,  9694, 17591,  7921, 12189, 17206, 12189, 13683, 11301,\n","        11986, 17537,  9788,  8368,  3871, 17537, 10472, 12189, 15061, 11676,\n","        16586, 17612, 18030,  6703, 12010, 17612, 17537,  6702, 17310, 17612,\n","        17537, 13681, 12560, 14150, 15746, 15587, 17537, 14867,  5012, 10590]), 'origin_page_id': tensor([2943]), 'destination_page_id': tensor([906]), 'label': tensor([1])}\n","\n","Tokens: ['It', 'extends', 'from', 'Dobruja', 'in', 'the', 'northeastern', 'corner', 'of', 'Bulgaria', 'and', 'southeastern', 'Romania', 'through', 'Moldova', 'and', 'southern', 'and', 'eastern', 'Ukraine', 'across', 'the', 'Russian', 'Northern', 'Caucasus', 'the', 'Southern', 'and', 'lower', 'Volga', 'regions', 'to', 'western', 'Kazakhstan', 'adjacent', 'to', 'the', 'Kazakh', 'steppe', 'to', 'the', 'east', 'both', 'forming', 'part', 'of', 'the', 'larger', 'Eurasian', 'Steppe']\n","OriginPage:   ['Pontic–Caspian steppe']\n","DestinationPage:   ['Ukraine']\n","Class:   ['P17']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frlS4lNILZMe","executionInfo":{"status":"ok","timestamp":1623727476618,"user_tz":420,"elapsed":196,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"35952e32-7bbe-499c-d427-25760aca9b2d"},"source":["print(len(train_dataset.sentence_token_vocab))"],"execution_count":106,"outputs":[{"output_type":"stream","text":["19315\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PCzBxcikLZMe"},"source":["### `DataLoader`"]},{"cell_type":"markdown","metadata":{"id":"yTvFaceOLZMg"},"source":["At this point our data is in a tensor, and we can create context windows using only PyTorch operations.\n","Now we need a way to generate batches of data for training and evaluation.\n","To do this, we will wrap our `Dataset` objects in a `torch.utils.data.DataLoader` object, which will automatically batch datapoints."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eal6bSh0LZMg","executionInfo":{"status":"ok","timestamp":1623727478056,"user_tz":420,"elapsed":240,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"e4c62102-23fe-4524-d743-89da4e6a1373"},"source":["batch_size = 1\n","print(f'Setting batch_size to be {batch_size}')\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size)\n","valdn_dataloader = torch.utils.data.DataLoader(valdn_dataset, batch_size)"],"execution_count":107,"outputs":[{"output_type":"stream","text":["Setting batch_size to be 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MQAWssttLZMr"},"source":["Now let's do one iteration over our training set to see what a batch looks like:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3IP3bq1LZMs","executionInfo":{"status":"ok","timestamp":1623727479515,"user_tz":420,"elapsed":156,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"1ee051cf-0f31-457e-8260-bed7447cf489"},"source":["for i,batch in enumerate(train_dataloader):\n","    print(batch, '\\n')\n","    print(f'Size of classes: {batch[\"label\"].size()}')\n","    if(i==2):\n","        break"],"execution_count":108,"outputs":[{"output_type":"stream","text":["{'sentence_token_ids': tensor([[ 6385, 13964, 14205,  4638, 14581, 17537, 15517, 13147, 15587,  3405,\n","         12189, 17205,  9694, 17591,  7921, 12189, 17206, 12189, 13683, 11301,\n","         11986, 17537,  9788,  8368,  3871, 17537, 10472, 12189, 15061, 11676,\n","         16586, 17612, 18030,  6703, 12010, 17612, 17537,  6702, 17310, 17612,\n","         17537, 13681, 12560, 14150, 15746, 15587, 17537, 14867,  5012, 10590]]), 'origin_page_id': tensor([[2943]]), 'destination_page_id': tensor([[906]]), 'label': tensor([[1]])} \n","\n","Size of classes: torch.Size([1, 1])\n","{'sentence_token_ids': tensor([[11684,  6989, 16294,  2454, 19149, 16839,  3057, 14858, 19297, 16840,\n","         14710, 11934, 15758, 13900,     0,  3057, 12827, 14581,  8015, 17140,\n","         15449, 17537, 12827, 15587,  7723, 12189, 12978, 13068, 12308, 17537,\n","         12187, 12683, 15587, 17537, 14783, 15587,  7665]]), 'origin_page_id': tensor([[4090]]), 'destination_page_id': tensor([[560]]), 'label': tensor([[1]])} \n","\n","Size of classes: torch.Size([1, 1])\n","{'sentence_token_ids': tensor([[10929,  9287, 15587,  8887,  6388,  9288, 13485,  8887, 14710, 17537,\n","         14867, 15587, 17537, 17753, 16404, 14581, 17537, 11316, 16582, 15587,\n","          6451, 13011, 17757, 15587, 12560, 17537, 12270, 12189, 15946, 15587,\n","         17537, 16582]]), 'origin_page_id': tensor([[3098]]), 'destination_page_id': tensor([[442]]), 'label': tensor([[1]])} \n","\n","Size of classes: torch.Size([1, 1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iTzTHSChLZMu"},"source":["## Model\n","\n","Now that we can read in the data, it is time to build our model.\n","We will build a very simple LSTM based tagger! Note that this is pretty similar to the code in `simple_tagger.py` in your homework, but with a lot of things hardcoded.\n","\n","Useful links:\n","- Embedding Layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","- LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","- Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear"]},{"cell_type":"code","metadata":{"id":"O1oOXMf8LZMu","executionInfo":{"status":"ok","timestamp":1623727483775,"user_tz":420,"elapsed":143,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["device = \"cuda\""],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"code_folding":[1,19],"id":"RqwpVp-mLZMu","executionInfo":{"status":"ok","timestamp":1623727485474,"user_tz":420,"elapsed":188,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["class LSTMClassifier(torch.nn.Module):\n","    def __init__(self, sentence_token_vocab, origin_page_vocab, destination_page_vocab, label_vocab):\n","        super(LSTMClassifier, self).__init__()\n","        self.sentence_token_vocab = sentence_token_vocab\n","        self.origin_page_vocab = origin_page_vocab\n","        self.destination_page_vocab = destination_page_vocab\n","        self.label_vocab = label_vocab\n","        self.num_classes = len(label_vocab) - 1          ##because of 1 special token in vocab\n","        # self.input_size=50\n","        self.hidden_size=25\n","        self.num_layers=1\n","        self.embedding_dim = 50\n","        \n","        # Initialize random embeddings of size 50 for each word in your token vocabulary\n","        self._embeddings = torch.nn.Embedding(len(sentence_token_vocab)+len(origin_page_vocab)+len(destination_page_vocab), self.embedding_dim)\n","        #print('Input dim: ', len(sentence_token_vocab)+len(origin_page_vocab)+len(destination_page_vocab))\n","        \n","        # Initialize a single-layer bidirectional LSTM encoder\n","        self._encoder = torch.nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True)\n","        \n","        # _encoder a Linear layer which projects from the hidden state size to the number of tags\n","        self._fc1 = torch.nn.Linear(in_features=2*self.hidden_size, out_features=self.hidden_size)\n","        self._fc2 = torch.nn.Linear(self.hidden_size, self.num_classes)\n","\n","        # Loss will be a Cross Entropy Loss over the tags (except the padding token)\n","        self.loss = torch.nn.MSELoss()\n","\n","    def forward(self, sentence_token_ids, origin_page_id, destination_page_id, label=None):\n","        # Create mask over all the positions where the input is padded\n","        #mask = token_ids != self.token_vocab.pad_token_id\n","        \n","        #Combine all the features\n","        input_ = sentence_token_ids.tolist().copy()\n","        for i, _ in enumerate(input_):\n","            input_[i].insert(0, destination_page_id[i][0])\n","            input_[i].insert(0, origin_page_id[i][0])\n","        input_ = torch.LongTensor(input_)\n","            \n","        #run on CUDA\n","#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        input_ = input_.to(device)\n","        #print(input_)\n","        \n","        # Embed Inputs\n","        embeddings = self._embeddings(input_).permute(1, 0, 2)                #######input_ is a 2d array\n","        #print(embeddings.shape)\n","        \n","        # Feed embeddings through LSTM\n","        encoder_outputs, (h_n, c_n) = self._encoder(embeddings)            #### why [0]?\n","        encoder_outputs = (encoder_outputs.permute(1, 0, 2))[:,-1,:]                          ##### choosing the last words encoding\n","        #print(encoder_outputs.shape)\n","        \n","        # Project output of LSTM through linear layer to get logits\n","        fc1_outputs = self._fc1(encoder_outputs)\n","        #print(fc1_outputs.shape)\n","        fc2_outputs = self._fc2(fc1_outputs)\n","        #print(fc2_outputs.shape)\n","        \n","        # Get the maximum score for each position as the predicted tag\n","        final_outputs = torch.nn.functional.softmax(fc2_outputs, dim=-1)\n","        preds = self.inverseOneHot(final_outputs)\n","        output_dict = {\n","            'predicted_labels':  preds\n","        }\n","        # Compute loss and accuracy if gold tags are provided\n","        if label is not None:\n","            label = label.to(device)\n","            target_labels = torch.Tensor(self.oneHotEncode(label)).to(device)\n","            #print(final_outputs.shape, target_labels.shape)\n","            loss = self.loss(final_outputs, target_labels)\n","            output_dict['loss'] = loss\n","            self.getAccuracy(target_labels, final_outputs, output_dict)\n","\n","        return output_dict\n","    \n","    def oneHotEncode(self, label):\n","        oneHot = np.zeros((label.shape[0],self.num_classes))\n","        for i,l in enumerate(label):\n","            oneHot[i][l[0]-1] = 1\n","        oneHot = oneHot.tolist()\n","        return oneHot\n","    \n","    def inverseOneHot(self, one_hot_labels):\n","        lbls = []\n","        for i in range(one_hot_labels.shape[0]):\n","            lbls.append(torch.argmax(one_hot_labels[i]))\n","        lbls = torch.Tensor(lbls)\n","        return lbls\n","        \n","    \n","    def getAccuracy(self, target_labels, final_outputs, output_dict):\n","        correct = torch.Tensor([1 if target_labels[i,torch.argmax(final_outputs[i]).to(int).item()].to(int).item()==1 else 0 for i in range(target_labels.shape[0])]) # 1's in positions where pred matches gold\n","        #correct *= mask # zero out positions where mask is zero\n","        output_dict['accuracy'] = (torch.sum(correct)/target_labels.shape[0]).item()\n","        \n","        #per label accuracies\n","        for i in range(target_labels.shape[0]):\n","            output_dict['label_accuracy'] = {}\n","            output_dict['label_count'] = {}\n","            \n","            cls = torch.argmax(final_outputs[i]).to(int).item()\n","            if cls not in output_dict['label_accuracy']:\n","                output_dict['label_accuracy'][cls] = 0.0\n","            if cls not in output_dict['label_count']:\n","                output_dict['label_count'][cls] = 0.0\n","            \n","            output_dict['label_count'][cls] += 1\n","            \n","            if target_labels[i,cls]==1:\n","                output_dict['label_accuracy'][cls] += 1\n","            else:\n","                output_dict['label_accuracy'][cls] += 0"],"execution_count":110,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YzIiCUOyLZMw"},"source":["### Training\n","\n","The training script essentially follows the same pattern that we used for the linear model above. However we have also added an evaluation step, and code for saving model checkpoints."]},{"cell_type":"code","metadata":{"code_folding":[26,45],"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"3HR49AqZLZMw","executionInfo":{"status":"ok","timestamp":1623727742348,"user_tz":420,"elapsed":250909,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"b24b32c2-2600-4a2c-c5fa-ea808af06512"},"source":["from tqdm import tqdm\n","\n","################################\n","# Setup\n","################################\n","# Create model\n","model = LSTMClassifier(sentence_token_vocab=sentence_token_vocab, origin_page_vocab=origin_page_vocab, destination_page_vocab=destination_page_vocab,label_vocab=label_vocab)\n","if device==\"cuda\":\n","    model = model.cuda()\n","\n","# Initialize optimizer.\n","# Note: The learning rate is an important hyperparameters to tune\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","################################\n","# Training and Evaluation!\n","################################\n","num_epochs = 10\n","# best_dev_loss = float('inf')\n","\n","for epoch in range(num_epochs):\n","    print('\\nEpoch', epoch)\n","    # Training loop\n","    model.train() # THIS PART IS VERY IMPORTANT TO SET BEFORE TRAINING\n","    train_loss = 0\n","    train_acc = 0\n","    label_acc, label_count = {}, {}\n","    for batch in train_dataloader:\n","        batch_size = batch['sentence_token_ids'].size(0)\n","        optimizer.zero_grad()\n","        output_dict = model(**batch)\n","        loss = output_dict['loss']\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()*batch_size\n","        accuracy = output_dict['accuracy']\n","        train_acc += accuracy*batch_size\n","        label_acc = {k: label_acc.get(k, 0) + output_dict['label_accuracy'].get(k, 0) for k in set(label_acc) | set(output_dict['label_accuracy'])}\n","        label_count = {k: label_count.get(k, 0) + output_dict['label_count'].get(k, 0) for k in set(label_count) | set(output_dict['label_count'])}\n","    train_loss /= len(train_dataset)\n","    train_acc /= len(train_dataset)\n","    print(f'Train loss {train_loss} accuracy {train_acc}')\n","#     for k in label_count:\n","#         print(f'{k} = {train_dataset.label_vocab.map_ids_to_tokens([k])} label accuracy {label_acc[k]/label_count[k]}')\n","    \n","    # Evaluation loop\n","    model.eval() # THIS PART IS VERY IMPORTANT TO SET BEFORE EVALUATION\n","    dev_loss = 0\n","    dev_acc = 0\n","    dev_label_acc, dev_label_count = {}, {}\n","    for batch in valdn_dataloader:\n","        batch_size = batch['sentence_token_ids'].size(0)\n","        output_dict = model(**batch)\n","        dev_loss += output_dict['loss'].item()*batch_size\n","        dev_acc += output_dict['accuracy']*batch_size\n","        dev_label_acc = {k: dev_label_acc.get(k, 0) + output_dict['label_accuracy'].get(k, 0) for k in set(dev_label_acc) | set(output_dict['label_accuracy'])}\n","        dev_label_count = {k: dev_label_count.get(k, 0) + output_dict['label_count'].get(k, 0) for k in set(dev_label_count) | set(output_dict['label_count'])}\n","    dev_loss /= len(valdn_dataset)\n","    dev_acc /= len(valdn_dataset)\n","    print(f'Dev loss {dev_loss} accuracy {dev_acc}')\n","    for k in dev_label_count:\n","        print(f'{k} = {valdn_dataset.label_vocab.map_ids_to_tokens([k])} label accuracy {dev_label_acc[k]/dev_label_count[k]}')\n","    \n","#     # Save best model\n","#     if dev_loss < best_dev_loss:\n","#         print('Best so far')\n","#         torch.save(model, 'model.pt')\n","#         best_dev_loss = dev_loss"],"execution_count":111,"outputs":[{"output_type":"stream","text":["\n","Epoch 0\n","Train loss 0.06613933239150539 accuracy 0.46813806837039496\n","Dev loss 0.05982429902930744 accuracy 0.496\n","0 = ['P131'] label accuracy 0.6163522012578616\n","1 = ['P17'] label accuracy 0.0\n","2 = ['P19'] label accuracy 0.0\n","3 = ['P20'] label accuracy 0.24\n","5 = ['P276'] label accuracy 0.3\n","7 = ['P39'] label accuracy 0.47619047619047616\n","8 = ['P47'] label accuracy 0.5\n","9 = ['P710'] label accuracy 0.0\n","\n","Epoch 1\n","Train loss 0.05269091696956827 accuracy 0.5814802522402921\n","Dev loss 0.05559320368650788 accuracy 0.58\n","0 = ['P131'] label accuracy 0.773109243697479\n","1 = ['P17'] label accuracy 0.3333333333333333\n","2 = ['P19'] label accuracy 0.16666666666666666\n","3 = ['P20'] label accuracy 0.21428571428571427\n","4 = ['P27'] label accuracy 0.5\n","5 = ['P276'] label accuracy 0.46153846153846156\n","6 = ['P361'] label accuracy 0.5\n","7 = ['P39'] label accuracy 0.6875\n","8 = ['P47'] label accuracy 0.4444444444444444\n","9 = ['P710'] label accuracy 0.42857142857142855\n","\n","Epoch 2\n","Train loss 0.042813045565303916 accuracy 0.6755725190839694\n","Dev loss 0.05051211493855362 accuracy 0.608\n","0 = ['P131'] label accuracy 0.8303571428571429\n","1 = ['P17'] label accuracy 0.2\n","2 = ['P19'] label accuracy 0.0\n","3 = ['P20'] label accuracy 0.19230769230769232\n","4 = ['P27'] label accuracy 1.0\n","5 = ['P276'] label accuracy 0.6\n","6 = ['P361'] label accuracy 0.6\n","7 = ['P39'] label accuracy 0.6875\n","8 = ['P47'] label accuracy 0.47058823529411764\n","9 = ['P710'] label accuracy 0.53125\n","\n","Epoch 3\n","Train loss 0.035268797128527485 accuracy 0.7407899103883173\n","Dev loss 0.0514283698757539 accuracy 0.608\n","0 = ['P131'] label accuracy 0.8640776699029126\n","1 = ['P17'] label accuracy 0.12903225806451613\n","2 = ['P19'] label accuracy 0.25\n","3 = ['P20'] label accuracy 0.3\n","4 = ['P27'] label accuracy 0.5\n","5 = ['P276'] label accuracy 0.6\n","6 = ['P361'] label accuracy 1.0\n","7 = ['P39'] label accuracy 0.75\n","8 = ['P47'] label accuracy 0.4117647058823529\n","9 = ['P710'] label accuracy 0.4878048780487805\n","\n","Epoch 4\n","Train loss 0.029063018218029812 accuracy 0.7923996017258547\n","Dev loss 0.049216215856277695 accuracy 0.62\n","0 = ['P131'] label accuracy 0.8378378378378378\n","1 = ['P17'] label accuracy 0.06666666666666667\n","2 = ['P19'] label accuracy 0.14285714285714285\n","3 = ['P20'] label accuracy 0.25\n","4 = ['P27'] label accuracy 0.3333333333333333\n","5 = ['P276'] label accuracy 0.5454545454545454\n","6 = ['P361'] label accuracy 0.6923076923076923\n","7 = ['P39'] label accuracy 0.75\n","8 = ['P47'] label accuracy 0.42105263157894735\n","9 = ['P710'] label accuracy 0.5277777777777778\n","\n","Epoch 5\n","Train loss 0.024808850918375994 accuracy 0.8264188516428809\n","Dev loss 0.054056810549871444 accuracy 0.592\n","0 = ['P131'] label accuracy 0.8725490196078431\n","1 = ['P17'] label accuracy 0.0\n","2 = ['P19'] label accuracy 0.25\n","3 = ['P20'] label accuracy 0.2222222222222222\n","4 = ['P27'] label accuracy 0.125\n","5 = ['P276'] label accuracy 0.5454545454545454\n","6 = ['P361'] label accuracy 1.0\n","7 = ['P39'] label accuracy 0.75\n","8 = ['P47'] label accuracy 0.3684210526315789\n","9 = ['P710'] label accuracy 0.5151515151515151\n","\n","Epoch 6\n","Train loss 0.02171464501545374 accuracy 0.8508131430467972\n","Dev loss 0.05539114962653127 accuracy 0.592\n","0 = ['P131'] label accuracy 0.8901098901098901\n","1 = ['P17'] label accuracy 0.0\n","2 = ['P19'] label accuracy 0.2\n","3 = ['P20'] label accuracy 0.29411764705882354\n","4 = ['P27'] label accuracy 0.14285714285714285\n","5 = ['P276'] label accuracy 0.6363636363636364\n","6 = ['P361'] label accuracy 0.75\n","7 = ['P39'] label accuracy 0.7058823529411765\n","8 = ['P47'] label accuracy 0.5384615384615384\n","9 = ['P710'] label accuracy 0.4523809523809524\n","\n","Epoch 7\n","Train loss 0.019368859667343818 accuracy 0.8690673747095917\n","Dev loss 0.053560096410266905 accuracy 0.632\n","0 = ['P131'] label accuracy 0.8910891089108911\n","1 = ['P17'] label accuracy 0.0625\n","2 = ['P19'] label accuracy 0.16666666666666666\n","3 = ['P20'] label accuracy 0.32\n","4 = ['P27'] label accuracy 0.2\n","5 = ['P276'] label accuracy 0.5555555555555556\n","6 = ['P361'] label accuracy 1.0\n","7 = ['P39'] label accuracy 0.8\n","8 = ['P47'] label accuracy 0.5714285714285714\n","9 = ['P710'] label accuracy 0.5263157894736842\n","\n","Epoch 8\n","Train loss 0.018094177908739507 accuracy 0.8776966478592765\n","Dev loss 0.057079168412729885 accuracy 0.624\n","0 = ['P131'] label accuracy 0.9010989010989011\n","1 = ['P17'] label accuracy 0.09090909090909091\n","2 = ['P19'] label accuracy 0.25\n","3 = ['P20'] label accuracy 0.47368421052631576\n","4 = ['P27'] label accuracy 0.2222222222222222\n","5 = ['P276'] label accuracy 0.5238095238095238\n","6 = ['P361'] label accuracy 0.8888888888888888\n","7 = ['P39'] label accuracy 0.7222222222222222\n","8 = ['P47'] label accuracy 0.38095238095238093\n","9 = ['P710'] label accuracy 0.48717948717948717\n","\n","Epoch 9\n","Train loss 0.01709298752236534 accuracy 0.8846664454032526\n","Dev loss 0.054626621407932535 accuracy 0.64\n","0 = ['P131'] label accuracy 0.9090909090909091\n","1 = ['P17'] label accuracy 0.1111111111111111\n","2 = ['P19'] label accuracy 0.18181818181818182\n","3 = ['P20'] label accuracy 0.34375\n","4 = ['P27'] label accuracy 0.15384615384615385\n","5 = ['P276'] label accuracy 0.6\n","6 = ['P361'] label accuracy 1.0\n","7 = ['P39'] label accuracy 0.7647058823529411\n","8 = ['P47'] label accuracy 0.5\n","9 = ['P710'] label accuracy 0.5454545454545454\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jrToLLg_LZMx"},"source":["test_batch_size = 1\n","test_sampler = torch.utils.data.SequentialSampler(test_dataset)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, sampler= test_sampler,batch_size= test_batch_size)\n","\n","# Evaluation loop\n","model.eval() # THIS PART IS VERY IMPORTANT TO SET BEFORE EVALUATION\n","with open('extended_datasets/predictions/sustainability_test_predictions.csv', mode='w') as f:\n","    for batch in test_dataloader:\n","        #print(batch)\n","        batch_size = batch['sentence_token_ids'].size(0)\n","        output_dict = model(**batch)\n","        #print(output_dict)\n","        test_wiki_labels = test_dataset.label_vocab.map_ids_to_tokens(output_dict['predicted_labels'].to(torch.int))\n","        for p in test_wiki_labels:\n","            f.write(p)\n","            f.write('\\n')\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E99s1dK-LZMy"},"source":["## Loading Trained Models\n","\n","Loading a pretrained model can be done easily. To learn more about saving/loading models see https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","metadata":{"id":"he9B3b9YLZMy"},"source":["model = torch.load('model.pt')\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","def oneHotToLabel(oneHot: list):\n","    return np.argmax(np.array(oneHot))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKoTMHU2LZMz"},"source":["## Feed in your own sentences!"]},{"cell_type":"code","metadata":{"id":"pAhSoIC7LZMz"},"source":["sentence = 'i want to eat a pizza .'.lower().split()\n","\n","# convert sentence to tensor dictionar\n","tensor_dict = train_dataset.tensorize(sentence)\n","\n","# unsqueeze first dimesion so batch size is 1\n","tensor_dict['token_ids'] = tensor_dict['token_ids'].unsqueeze(0)\n","tensor_dict['label'] = None\n","print(tensor_dict)\n","\n","# feed through model\n","output_dict = model(**tensor_dict)\n","\n","# get predicted tag IDs\n","pred_label = output_dict['predicted_label'].squeeze().tolist()\n","print(len(pred_label))\n","\n","# convert tag IDs to tag names\n","print(oneHotToLabel(pred_label))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C4TubKe6LZM1"},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{"id":"_NphT1FyLZM1"},"source":["You've now seen at a high level how to create neural networks for NLP.\n","You've also now seen the components that go around a model (e.g. training loops, data processing).\n","Setting up these componenents in a flexible way can be tricky for NLP, as there are many issues that you have to take care of like padding, different vocabularies, etc.\n","For example, how would you build upon this code to load in pre-trained embeddings, or use character embeddings?\n","\n","That's why there exist many libraries that take care of these boilerplate components so that you can focus on modeling.\n","One of these libraries is [allennlp](https://allennlp.org/), and if you have time, I encourage you to take a look at it. \n","It builds upon PyTorch so everything you've learned here is applicable."]}]}