{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY: POS Tagging!\n",
    "\n",
    "Now let's dive into an example that is more relevant to NLP and is relevant to your HW3, part-of-speech tagging! We will be building up code up until the point where you will be able to process the POS data into tensors, then train a simple model on it.\n",
    "The code we are building up to forms the basis of the code in the homework assignment.\n",
    "\n",
    "To start, we'll need some data to train and evaluate on. First download the train and dev POS data `twitter_train.pos` and `twitter_dev.pos` into the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be introducing three new components which are vital to training (NLP) models:\n",
    "1. a `Vocabulary` object which converts from tokens/labels to integers. This part should also be able to handle padding so that batches can be easily created.\n",
    "2. a `Dataset` object which takes in the data file and produces data tensors\n",
    "3. a `DataLoader` object which takes data tensors from `Dataset` and batches them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Vocabulary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get our data into Python and in a form that is usable by PyTorch. For text data this typically entails building a `Vocabulary`  of all of the words, then mapping words to integers corresponding to their place in the sorted vocabulary. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     6,
     23,
     28,
     31,
     37,
     41,
     49
    ]
   },
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    \"\"\" Object holding vocabulary and mappings\n",
    "    Args:\n",
    "        word_list: ``list`` A list of words. Words assumed to be unique.\n",
    "        add_unk_token: ``bool` Whether to create an token for unknown tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_list, add_unk_token=False):\n",
    "        # create special tokens for padding and unknown words\n",
    "        self.pad_token = '<pad>'\n",
    "        self.unk_token = '<unk>' if add_unk_token else None\n",
    "\n",
    "        self.special_tokens = [self.pad_token]\n",
    "        if self.unk_token:\n",
    "            self.special_tokens += [self.unk_token]\n",
    "\n",
    "        self.word_list = word_list\n",
    "        \n",
    "        # maps from the token ID to the token\n",
    "        self.id_to_token = self.word_list + self.special_tokens\n",
    "        # maps from the token to its token ID\n",
    "        self.token_to_id = {token: id for id, token in\n",
    "                            enumerate(self.id_to_token)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Returns size of vocabulary \"\"\"\n",
    "        return len(self.token_to_id)\n",
    "    \n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self.map_token_to_id(self.pad_token)\n",
    "        \n",
    "    def map_token_to_id(self, token: str):\n",
    "        \"\"\" Maps a single token to its token ID \"\"\"\n",
    "        if token not in self.token_to_id:\n",
    "            token = self.unk_token\n",
    "        return self.token_to_id[token]\n",
    "\n",
    "    def map_id_to_token(self, id: int):\n",
    "        \"\"\" Maps a single token ID to its token \"\"\"\n",
    "        return self.id_to_token[id]\n",
    "\n",
    "    def map_tokens_to_ids(self, tokens: list, max_length: int = None):\n",
    "        \"\"\" Maps a list of tokens to a list of token IDs \"\"\"\n",
    "        # truncate extra tokens and pad to `max_length`\n",
    "        if max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "            tokens = tokens + [self.pad_token]*(max_length-len(tokens))\n",
    "        return [self.map_token_to_id(token) for token in tokens]\n",
    "\n",
    "    def map_ids_to_tokens(self, ids: list, filter_padding=True):\n",
    "        \"\"\" Maps a list of token IDs to a list of token \"\"\"\n",
    "        tokens = [self.map_id_to_token(id) for id in ids]\n",
    "        if filter_padding:\n",
    "            tokens = [t for t in tokens if t != self.pad_token]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a way to efficiently read in the data file and to process it into tensors. PyTorch provides an easy way to do this using the `torch.utils.data.Dataset` class. We will be creating our own class which inherits from this class. \n",
    "\n",
    "Helpful link: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom `Dataset` class must implement three functions: \n",
    "\n",
    "- $__init__$: The init functions is run once when instantisting the `Dataset` object.\n",
    "- $__len__$: The len function returns the number of data points in our dataset.\n",
    "- $__getitem__$. The getitem function returns a sample from the dataset give the index of the sample. The output of this part should be a dictionary of (mostly) PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     22,
     25,
     36,
     45,
     50,
     55
    ]
   },
   "outputs": [],
   "source": [
    "class WikiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self._dataset = []\n",
    "    \n",
    "        # read the dataset file, extracting tokens and tags\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for i,line in enumerate(f):\n",
    "                if(i==0): continue\n",
    "                tokens = []\n",
    "                elements = line.strip().split(',')          #############################################\n",
    "                content = elements[3].split(' ')\n",
    "                for word in content:\n",
    "                    clean_word = word.replace(\".\", \"\").replace(\"(\",\"\").replace(\")\",\"\")\n",
    "                    tokens.append(clean_word)\n",
    "                #elements[-2] = elements[-2].replace(\"P\",\"\")\n",
    "                self._dataset.append({'tokens': tokens, 'label': [elements[-2]]})\n",
    "        \n",
    "        # intiailize an empty vocabulary\n",
    "        self.token_vocab = None\n",
    "        self.label_vocab = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        # get the sample corresponding to the index\n",
    "        instance = self._dataset[item]\n",
    "        \n",
    "        # check the vocabulary has been set\n",
    "        assert self.token_vocab is not None\n",
    "        assert self.label_vocab is not None\n",
    "        \n",
    "        # Convert inputs to tensors, then return\n",
    "        return self.tensorize(instance['tokens'], instance['label'])\n",
    "    \n",
    "    def tensorize(self, tokens, cls=None, max_length=None):\n",
    "        # map the tokens and tags into their ID form\n",
    "        token_ids = self.token_vocab.map_tokens_to_ids(tokens, max_length)\n",
    "        tensor_dict = {'token_ids': torch.LongTensor(token_ids)}\n",
    "        if cls:\n",
    "            label_map = self.label_vocab.map_tokens_to_ids(cls)\n",
    "            tensor_dict['label'] = torch.LongTensor(label_map)\n",
    "        return tensor_dict\n",
    "        \n",
    "    def get_tokens_list(self):\n",
    "        \"\"\" Returns set of tokens in dataset \"\"\"\n",
    "        tokens = [token for d in self._dataset for token in d['tokens']]\n",
    "        return sorted(set(tokens))\n",
    "\n",
    "    def get_classes_list(self):\n",
    "        \"\"\" Returns set of tags in dataset \"\"\"\n",
    "        clss = [c for d in self._dataset for c in d['label']]\n",
    "        return sorted(set(clss))\n",
    "\n",
    "    def set_vocab(self, token_vocab: Vocabulary, label_vocab: Vocabulary):\n",
    "        self.token_vocab = token_vocab\n",
    "        self.label_vocab = label_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create `Dataset` objects for our training and validation sets!\n",
    "A key step here is creating the `Vocabulary` for these datasets.\n",
    "We will use the list of words in the training set to intialize a `Vocabulary` object over the input words. \n",
    "We will also use list of tags to intialize a `Vocabulary` over the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WikiDataset('basketball_links_results_training_dedup.csv')\n",
    "dev_dataset = WikiDataset('basketball_links_results_training_dedup.csv')     ###################################################\n",
    "\n",
    "# Get list of tokens and tags seen in training set and use to create Vocabulary\n",
    "token_list = train_dataset.get_tokens_list()\n",
    "class_list = train_dataset.get_classes_list()\n",
    "\n",
    "token_vocab = Vocabulary(token_list, add_unk_token=True)\n",
    "label_vocab = Vocabulary(class_list, add_unk_token=True)\n",
    "\n",
    "# Update the train/dev set with vocabulary. Notice we created the vocabulary using the training set\n",
    "train_dataset.set_vocab(token_vocab, label_vocab)\n",
    "dev_dataset.set_vocab(token_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 2683\n",
      "Size of validation set: 2683\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of training set: {len(train_dataset)}')\n",
    "print(f'Size of validation set: {len(dev_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out one data point of the tensorized data and see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': tensor([2294, 5427, 6330, 6171, 5305, 6062, 5717, 4757, 6330, 2957,  849, 3743,\n",
      "        1599]), 'label': tensor([61])}\n",
      "\n",
      "Tokens: ['It', 'is', 'the', 'sole', 'high', 'school', 'operated', 'by', 'the', 'Montgomery', 'Area', 'School', 'District']\n",
      "Class:   ['P5353']\n"
     ]
    }
   ],
   "source": [
    "instance = train_dataset[0]\n",
    "print(instance)\n",
    "\n",
    "tokens = train_dataset.token_vocab.map_ids_to_tokens(instance['token_ids'])\n",
    "cls = train_dataset.label_vocab.map_ids_to_tokens(instance['label'])\n",
    "print()\n",
    "print(f'Tokens: {tokens}')\n",
    "print(f'Class:   {cls}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our data is in a tensor, and we can create context windows using only PyTorch operations.\n",
    "Now we need a way to generate batches of data for training and evaluation.\n",
    "To do this, we will wrap our `Dataset` objects in a `torch.utils.data.DataLoader` object, which will automatically batch datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting batch_size to be 1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "print(f'Setting batch_size to be {batch_size}')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size)\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do one iteration over our training set to see what a batch looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': tensor([[2294, 5427, 6330, 6171, 5305, 6062, 5717, 4757, 6330, 2957,  849, 3743,\n",
      "         1599]]), 'label': tensor([[61]])} \n",
      "\n",
      "Size of classes: torch.Size([1, 1])\n",
      "{'token_ids': tensor([[2584, 1077, 5427, 6330, 5711, 5945, 6372, 5288, 4595, 4699, 5630, 2737,\n",
      "         4642, 4532, 5811]]), 'label': tensor([[12]])} \n",
      "\n",
      "Size of classes: torch.Size([1, 1])\n",
      "{'token_ids': tensor([[229]]), 'label': tensor([[21]])} \n",
      "\n",
      "Size of classes: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(train_dataloader):\n",
    "    print(batch, '\\n')\n",
    "    print(f'Size of classes: {batch[\"label\"].size()}')\n",
    "    if(i==2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now that we can read in the data, it is time to build our model.\n",
    "We will build a very simple LSTM based tagger! Note that this is pretty similar to the code in `simple_tagger.py` in your homework, but with a lot of things hardcoded.\n",
    "\n",
    "Useful links:\n",
    "- Embedding Layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "- LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "- Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     1,
     19
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleTagger(torch.nn.Module):\n",
    "    def __init__(self, token_vocab, label_vocab):\n",
    "        super(SimpleTagger, self).__init__()\n",
    "        self.token_vocab = token_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.num_classes = len(label_vocab) - 2          ##because of 2 special tokens in vocab\n",
    "        self.input_size=50\n",
    "        self.hidden_size=25\n",
    "        self.num_layers=1\n",
    "        self.embedding_dim = 50\n",
    "        \n",
    "        # Initialize random embeddings of size 50 for each word in your token vocabulary\n",
    "        self._embeddings = torch.nn.Embedding(len(token_vocab), self.embedding_dim)\n",
    "        \n",
    "        # Initialize a single-layer bidirectional LSTM encoder\n",
    "        self._encoder = torch.nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True)\n",
    "        \n",
    "        # _encoder a Linear layer which projects from the hidden state size to the number of tags\n",
    "        self._fc1 = torch.nn.Linear(in_features=2*self.hidden_size, out_features=self.hidden_size)\n",
    "        self._fc2 = torch.nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "        # Loss will be a Cross Entropy Loss over the tags (except the padding token)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, token_ids, label):\n",
    "        # Create mask over all the positions where the input is padded\n",
    "        #mask = token_ids != self.token_vocab.pad_token_id\n",
    "        #run on CUDA\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        token_ids = token_ids.to(device)\n",
    "        # Embed Inputs\n",
    "        #print(token_ids.shape)\n",
    "        embeddings = self._embeddings(token_ids).permute(1, 0, 2)                #######token ids is a 2d array\n",
    "        #print(embeddings.shape)\n",
    "        # Feed embeddings through LSTM\n",
    "        encoder_outputs = self._encoder(embeddings)[0].permute(1, 0, 2)     #### why [0]?\n",
    "        encoder_outputs = encoder_outputs[:,-1,:]                          ##### choosing the last words encoding\n",
    "        #print(encoder_outputs.shape)\n",
    "        # Project output of LSTM through linear layer to get logits\n",
    "        fc1_outputs = self._fc1(encoder_outputs)\n",
    "        #print(fc1_outputs.shape)\n",
    "        fc2_outputs = self._fc2(fc1_outputs)\n",
    "        #print(fc2_outputs.shape)\n",
    "        # Get the maximum score for each position as the predicted tag\n",
    "        final_outputs = torch.nn.functional.softmax(fc2_outputs, dim=-1)\n",
    "        \n",
    "        output_dict = {\n",
    "            'predicted_label':  final_outputs  # convert values to probs\n",
    "        }\n",
    "        # Compute loss and accuracy if gold tags are provided\n",
    "        if label is not None:\n",
    "            label = label.to(device)\n",
    "            target_labels = torch.Tensor(self.oneHotEncode(label)).to(device)\n",
    "            #print(final_outputs.shape, target_labels.shape)\n",
    "            loss = self.loss(final_outputs, target_labels)\n",
    "            output_dict['loss'] = loss\n",
    "\n",
    "            correct = torch.Tensor([1 if target_labels[i,torch.argmax(final_outputs[i])]==1 else 0 for i in range(target_labels.shape[0])]) # 1's in positions where pred matches gold\n",
    "            #correct *= mask # zero out positions where mask is zero\n",
    "            output_dict['accuracy'] = torch.sum(correct)/target_labels.shape[0]\n",
    "\n",
    "        return output_dict\n",
    "    \n",
    "    def oneHotEncode(self, label):\n",
    "        oneHot = np.zeros((label.shape[0],self.num_classes))\n",
    "        for i,l in enumerate(label):\n",
    "            oneHot[i][l[0]-1] = 1\n",
    "        oneHot = oneHot.tolist()\n",
    "        return oneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training script essentially follows the same pattern that we used for the linear model above. However we have also added an evaluation step, and code for saving model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     26,
     45
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "Train loss 0.009017906530820468 accuracy 0.44353336095809937\n",
      "Dev loss 0.007407454932819451 accuracy 0.5393216609954834\n",
      "Best so far\n",
      "\n",
      "Epoch 1\n",
      "Train loss 0.007190224206696131 accuracy 0.5672754645347595\n",
      "Dev loss 0.0062656072959730415 accuracy 0.6306373476982117\n",
      "Best so far\n",
      "\n",
      "Epoch 2\n",
      "Train loss 0.006187825436581479 accuracy 0.6407006978988647\n",
      "Dev loss 0.005438254338092773 accuracy 0.6865448951721191\n",
      "Best so far\n",
      "\n",
      "Epoch 3\n",
      "Train loss 0.005405245733449477 accuracy 0.6861721873283386\n",
      "Dev loss 0.004799660144017199 accuracy 0.7204621434211731\n",
      "Best so far\n",
      "\n",
      "Epoch 4\n",
      "Train loss 0.004775789404262164 accuracy 0.7144986987113953\n",
      "Dev loss 0.004329323236068431 accuracy 0.752888560295105\n",
      "Best so far\n",
      "\n",
      "Epoch 5\n",
      "Train loss 0.004275633770945493 accuracy 0.7517703771591187\n",
      "Dev loss 0.003977741912409904 accuracy 0.7707789540290833\n",
      "Best so far\n",
      "\n",
      "Epoch 6\n",
      "Train loss 0.004106253581638715 accuracy 0.7562429904937744\n",
      "Dev loss 0.0037885960976154604 accuracy 0.7815877795219421\n",
      "Best so far\n",
      "\n",
      "Epoch 7\n",
      "Train loss 0.003819129752165245 accuracy 0.7722698450088501\n",
      "Dev loss 0.003609140994262647 accuracy 0.7972418665885925\n",
      "Best so far\n",
      "\n",
      "Epoch 8\n",
      "Train loss 0.003637022148605513 accuracy 0.7860603928565979\n",
      "Dev loss 0.0036704338160259277 accuracy 0.7927693128585815\n",
      "\n",
      "Epoch 9\n",
      "Train loss 0.003470348376564554 accuracy 0.8017144799232483\n",
      "Dev loss 0.003331563962461499 accuracy 0.8169959187507629\n",
      "Best so far\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "################################\n",
    "# Setup\n",
    "################################\n",
    "# Create model\n",
    "model = SimpleTagger(token_vocab=token_vocab, label_vocab=label_vocab)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Initialize optimizer.\n",
    "# Note: The learning rate is an important hyperparameters to tune\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "################################\n",
    "# Training and Evaluation!\n",
    "################################\n",
    "num_epochs = 10\n",
    "best_dev_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('\\nEpoch', epoch)\n",
    "    # Training loop\n",
    "    model.train() # THIS PART IS VERY IMPORTANT TO SET BEFORE TRAINING\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch_size = batch['token_ids'].size(0)\n",
    "        optimizer.zero_grad()\n",
    "        output_dict = model(**batch)\n",
    "        loss = output_dict['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*batch_size\n",
    "        accuracy = output_dict['accuracy']\n",
    "        train_acc += accuracy*batch_size\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc /= len(train_dataset)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    # Evaluation loop\n",
    "    model.eval() # THIS PART IS VERY IMPORTANT TO SET BEFORE EVALUATION\n",
    "    dev_loss = 0\n",
    "    dev_acc = 0\n",
    "    for batch in dev_dataloader:\n",
    "        batch_size = batch['token_ids'].size(0)\n",
    "        output_dict = model(**batch)\n",
    "        dev_loss += output_dict['loss'].item()*batch_size\n",
    "        dev_acc += output_dict['accuracy']*batch_size\n",
    "    dev_loss /= len(dev_dataset)\n",
    "    dev_acc /= len(dev_dataset)\n",
    "    print(f'Dev loss {dev_loss} accuracy {dev_acc}')\n",
    "    \n",
    "    # Save best model\n",
    "    if dev_loss < best_dev_loss:\n",
    "        print('Best so far')\n",
    "        torch.save(model, 'model.pt')\n",
    "        best_dev_loss = dev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of classes:  ['P1056', 'P106', 'P108', 'P115', 'P118', 'P123', 'P1268', 'P1269', 'P127', 'P131', 'P1313', 'P1344', 'P1346', 'P1365', 'P1366', 'P137', 'P1376', 'P138', 'P1416', 'P150', 'P155', 'P156', 'P159', 'P166', 'P17', 'P172', 'P178', 'P179', 'P1830', 'P1889', 'P19', 'P190', 'P20', 'P22', 'P2354', 'P2499', 'P2500', 'P2596', 'P27', 'P276', 'P279', 'P286', 'P30', 'P31', 'P3373', 'P3450', 'P36', 'P360', 'P361', 'P3842', 'P39', 'P40', 'P413', 'P449', 'P463', 'P466', 'P47', 'P495', 'P5125', 'P527', 'P530', 'P5353', 'P54', 'P551', 'P6087', 'P641', 'P647', 'P664', 'P69', 'P740', 'P7888', 'P793', 'P840', 'P937']\n",
      "74\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Device() received an invalid combination of arguments - got (), but expected one of:\n * (torch.device device)\n * (str type, int index)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ff3eaf4f624a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Device() received an invalid combination of arguments - got (), but expected one of:\n * (torch.device device)\n * (str type, int index)\n"
     ]
    }
   ],
   "source": [
    "print('num of classes: ',(class_list))\n",
    "print(model.num_classes)\n",
    "torch.cuda.is_available()\n",
    "torch.device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Trained Models\n",
    "\n",
    "Loading a pretrained model can be done easily. To learn more about saving/loading models see https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt')\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "def oneHotToLabel(oneHot: list):\n",
    "    return np.argmax(np.array(oneHot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed in your own sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': tensor([[3149, 4439, 4151, 4439, 2248, 4439, 4439]], device='cpu'), 'label': None}\n",
      "torch.Size([1, 7])\n",
      "torch.Size([7, 1, 50])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 6379])\n",
      "6379\n",
      "936\n"
     ]
    }
   ],
   "source": [
    "sentence = 'i want to eat a pizza .'.lower().split()\n",
    "\n",
    "# convert sentence to tensor dictionar\n",
    "tensor_dict = train_dataset.tensorize(sentence)\n",
    "\n",
    "# unsqueeze first dimesion so batch size is 1\n",
    "tensor_dict['token_ids'] = tensor_dict['token_ids'].unsqueeze(0)\n",
    "tensor_dict['label'] = None\n",
    "print(tensor_dict)\n",
    "\n",
    "# feed through model\n",
    "output_dict = model(**tensor_dict)\n",
    "\n",
    "# get predicted tag IDs\n",
    "pred_label = output_dict['predicted_label'].squeeze().tolist()\n",
    "print(len(pred_label))\n",
    "\n",
    "# convert tag IDs to tag names\n",
    "print(oneHotToLabel(pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now seen at a high level how to create neural networks for NLP.\n",
    "You've also now seen the components that go around a model (e.g. training loops, data processing).\n",
    "Setting up these componenents in a flexible way can be tricky for NLP, as there are many issues that you have to take care of like padding, different vocabularies, etc.\n",
    "For example, how would you build upon this code to load in pre-trained embeddings, or use character embeddings?\n",
    "\n",
    "That's why there exist many libraries that take care of these boilerplate components so that you can focus on modeling.\n",
    "One of these libraries is [allennlp](https://allennlp.org/), and if you have time, I encourage you to take a look at it. \n",
    "It builds upon PyTorch so everything you've learned here is applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
