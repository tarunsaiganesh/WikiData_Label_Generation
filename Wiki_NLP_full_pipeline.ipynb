{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2556cf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Wikidata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7% (3661 of 49696) |#                  | Elapsed Time: 0:02:44 ETA:   0:40:03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection timed out and retrying URL:  https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=select%20%3Fs%20%3Fp%20%3Fo%20where%20%7B%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ12560%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ12560%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ822%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ822%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ71084%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ71084%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ173065%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ173065%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ142%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ142%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ4948%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ4948%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ21%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ21%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ12548%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ12548%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ170174%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ170174%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ21%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ21%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ29%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ29%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ17036918%7D%20VALUES%20%3Fo%20%7Bwd%3AQ12560%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ12560%7D%20VALUES%20%3Fo%20%7Bwd%3AQ17036918%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ1642171%7D%20VALUES%20%3Fo%20%7Bwd%3AQ4040%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ4040%7D%20VALUES%20%3Fo%20%7Bwd%3AQ1642171%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ1642171%7D%20VALUES%20%3Fo%20%7Bwd%3AQ10965%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ10965%7D%20VALUES%20%3Fo%20%7Bwd%3AQ1642171%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ1642171%7D%20VALUES%20%3Fo%20%7Bwd%3AQ2807%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ2807%7D%20VALUES%20%3Fo%20%7Bwd%3AQ1642171%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ830730%7D%20VALUES%20%3Fo%20%7Bwd%3AQ183%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ183%7D%20VALUES%20%3Fo%20%7Bwd%3AQ830730%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ1327022%7D%20VALUES%20%3Fo%20%7Bwd%3AQ29%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ29%7D%20VALUES%20%3Fo%20%7Bwd%3AQ1327022%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ1327022%7D%20VALUES%20%3Fo%20%7Bwd%3AQ5011445%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ5011445%7D%20VALUES%20%3Fo%20%7Bwd%3AQ1327022%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ1327022%7D%20VALUES%20%3Fo%20%7Bwd%3AQ190992%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ190992%7D%20VALUES%20%3Fo%20%7Bwd%3AQ1327022%7D%20%3Fs%20%3Fp%20%3Fo%20%7DUNION%20%7BVALUES%20%3Fs%20%7Bwd%3AQ1327022%7D%20VALUES%20%3Fo%20%7Bwd%3AQ1747689%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%20UNION%20%7B%20VALUES%20%3Fs%20%7Bwd%3AQ1747689%7D%20VALUES%20%3Fo%20%7Bwd%3AQ1327022%7D%20%3Fs%20%3Fp%20%3Fo%20%7D%7D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7% (3801 of 49696) |#                  | Elapsed Time: 0:03:11 ETA:   0:30:14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Metelli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% (5161 of 49696) |#                  | Elapsed Time: 0:04:13 ETA:   0:33:46"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Maecenas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14% (7181 of 49696) |##                 | Elapsed Time: 0:05:43 ETA:   0:33:40"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Alexander VI\n",
      "Failed to find Wikidata ID for  Alexander VI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15% (7601 of 49696) |##                 | Elapsed Time: 0:06:02 ETA:   0:36:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Occitan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28% (14221 of 49696) |#####             | Elapsed Time: 0:11:05 ETA:   0:26:20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Alexander VI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36% (18141 of 49696) |######            | Elapsed Time: 0:14:05 ETA:   0:19:52"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Founding myth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37% (18841 of 49696) |######            | Elapsed Time: 0:14:36 ETA:   0:24:57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Occitan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38% (18981 of 49696) |######            | Elapsed Time: 0:14:42 ETA:   0:23:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Maecenas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39% (19561 of 49696) |#######           | Elapsed Time: 0:15:07 ETA:   0:22:25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Gaius Calpurnius Piso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42% (20921 of 49696) |#######           | Elapsed Time: 0:16:10 ETA:   0:19:19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Palazzo Nuovo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51% (25681 of 49696) |#########         | Elapsed Time: 0:19:46 ETA:   0:18:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Occitan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52% (26241 of 49696) |#########         | Elapsed Time: 0:20:09 ETA:   0:15:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find Wikidata ID for  Gaius Calpurnius Piso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55% (27521 of 49696) |#########         | Elapsed Time: 0:21:05 ETA:   0:16:24"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import string\n",
    "import re\n",
    "import urllib.parse\n",
    "import csv\n",
    "import time\n",
    "from py2neo import Graph\n",
    "from queue import Queue\n",
    "import progressbar\n",
    "import numpy as np\n",
    "import os\n",
    "import pyparsing as pp\n",
    "\n",
    "def runWithRetry(query):\n",
    "    try:\n",
    "        return graph.run(query)\n",
    "    except Exception as e:\n",
    "        print(e.args[0])\n",
    "        time.sleep(1)\n",
    "        return runWithRetry(query)\n",
    "\n",
    "def getWithRetry(url):\n",
    "    try:\n",
    "        return requests.get(url, headers = {\"User-Agent\":\"Wiki_NLP/0.0 (https://github.com/greenguy33/wikidata-subgraph-builder; hfreedma@uci.edu)\"})\n",
    "    except Exception:\n",
    "        print(\"Connection timed out and retrying URL: \", url)\n",
    "        time.sleep(1)\n",
    "        return getWithRetry(url)\n",
    "    \n",
    "#graph = Graph(\"http://localhost:7474/db/data/\")\n",
    "    \n",
    "def main():\n",
    "    output_filename = \"roman_history\"\n",
    "    primaryDomain = \"History of Rome\"\n",
    "    repo_name = \"na\"\n",
    "    \n",
    "    if os.path.exists(\"data/output/\"+output_filename+\"_all_results.txt\") == False:\n",
    "        #strongLinks = getStrongLinks(primaryDomain)\n",
    "        strongLinks = [primaryDomain]\n",
    "        #strongLinks.remove(\"Rome\")\n",
    "        networkHops = 1\n",
    "        networkList, redirectMap = getWikipediaNetworkList(strongLinks, networkHops)\n",
    "        linkData = parseWikipediaPagesInNetwork(networkList, redirectMap)\n",
    "\n",
    "        with open(\"data/output/\"+output_filename+\"_all_results.txt\", 'w', newline='', encoding = \"utf-8\") as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerows(linkData)\n",
    "    \n",
    "    linkData = []\n",
    "    with open(\"data/output/\"+output_filename+\"_all_results.txt\", 'r', encoding = \"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in reader:\n",
    "            linkData.append(row)\n",
    "    \n",
    "    training_data, test_data = checkWikidataForConnections(linkData, repo_name)\n",
    "    \n",
    "    print(\"final training size: \", len(training_data))\n",
    "    headers = ['Origin Page','Destination Page','Link Text','Sentence Text','Wikidata Property Label','Direction']\n",
    "    with open(\"data/output/training/\"+output_filename+\"_training.txt\", 'w', newline='', encoding = \"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(training_data)\n",
    "\n",
    "    print(\"final test size: \", len(test_data))\n",
    "    headers = ['Origin Page','Destination Page','Link Text','Sentence Text']\n",
    "    with open(\"data/output/test/\"+output_filename+\"_test.txt\", 'w', newline='', encoding = \"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(test_data)\n",
    "\n",
    "# Neo4j Cypher lookup\n",
    "# Handles page redirects cleanly\n",
    "def getWikipediaNetworkList(domains, networkHops):\n",
    "    param_query = \"MATCH (source:Page {title: \\\"subj\\\"}) {direction} (target:Page) RETURN target.title\"\n",
    "    networkList = set()\n",
    "    redirectMap = {}\n",
    "    for domain in domains:\n",
    "        queue = Queue()\n",
    "        queue.put({domain:0})\n",
    "        networkList.add(domain)\n",
    "        while(queue.empty() == False):\n",
    "            nextItem = queue.get()\n",
    "            element = next(iter(nextItem))\n",
    "            layer = nextItem[element]\n",
    "            if layer >= networkHops:\n",
    "                break\n",
    "            query = param_query.replace(\"subj\", element.replace(\"\\\"\",\"\\\\\\\"\")).replace(\"{direction}\", \"- [:Link] -\")\n",
    "            query_data = runWithRetry(query).data()\n",
    "            for res in query_data:\n",
    "                page = res[\"target.title\"]\n",
    "                # Get only forward links to see if page is a redirect\n",
    "                query = param_query.replace(\"subj\", page.replace(\"\\\"\",\"\\\\\\\"\")).replace(\"{direction}\", \"- [:Link] -> \")\n",
    "                query_data = runWithRetry(query).data()\n",
    "                if (len(query_data) == 1):\n",
    "                    redirected_page = query_data[0][\"target.title\"]\n",
    "                    #print(\"Found redirect: \", page, \" to \", redirected_page)\n",
    "                    redirectMap[page] = redirected_page\n",
    "                    page = redirected_page\n",
    "                networkList.add(page)\n",
    "                queue.put({page:layer+1})\n",
    "    print(\"Total network size: \", str(len(networkList)))\n",
    "    return networkList, redirectMap\n",
    "\n",
    "# Request and parse Wikipedia HTML\n",
    "# Only parses introduction/abstract section of pages\n",
    "# A shortcoming is that this code will fail to parse link text with a . in it\n",
    "def parseWikipediaPagesInNetwork(networkList, redirectMap):\n",
    "    linkData = []\n",
    "    for line in progressbar.progressbar(networkList, redirect_stdout=True):\n",
    "        if line in redirectMap:\n",
    "            line = redirectMap[line]\n",
    "            print(\"Redirected to: \", line)\n",
    "        pageTitle = urllib.parse.quote(line)\n",
    "        getUrl = f\"https://en.wikipedia.org/w/api.php?action=parse&page={pageTitle}&format=json\"\n",
    "        jsonRes = getWithRetry(getUrl).json()\n",
    "        if 'parse' not in jsonRes:\n",
    "            print(\"Could not retrieve data for page: \", line)\n",
    "        else:\n",
    "            jsonRes = jsonRes['parse']['text']['*']\n",
    "            absStart = jsonRes.split(\"<p>\", 1)\n",
    "            if len(absStart) == 1:\n",
    "                print(\"Unable to parse page: \", line)\n",
    "            else:\n",
    "                absText = absStart[1].split(\"<h2\")[0]\n",
    "                sentences = re.split(r'\\n|\\. |\\.<sup id=', absText)\n",
    "                for i in sentences:\n",
    "                    linkSplit = i.split(\"<a href=\")\n",
    "                    if len(linkSplit) > 1:\n",
    "                        for link in linkSplit:\n",
    "                            if link.startswith(\"\\\"/wiki/\"):\n",
    "                                destinationPage = link.split(\"\\\"/wiki/\")[1].split(\"\\\"\")[0]\n",
    "                                if destinationPage.replace(\"_\",\" \") in networkList:\n",
    "                                    if \">\" not in link or \"<\" not in link:\n",
    "                                        print(\"failed to parse link text: \", link)\n",
    "                                    else:\n",
    "                                        linkTitle = link.split(\">\")[1].split(\"<\")[0]\n",
    "                                        sentString = re.sub('<[^>]+>', '', i)\n",
    "                                        sentString = sentString.translate(str.maketrans('', '', string.punctuation))\n",
    "                                        sentString = re.sub('91[0-9]+93', '', sentString)\n",
    "                                        destinationPage = destinationPage.replace(\"|\",\"\")\n",
    "                                        linkTitle = linkTitle.replace(\"|\",\"\")\n",
    "                                        if sentString.startswith(\"citeref\"):\n",
    "                                            sentString = sentString.split(\" \",2)[2]\n",
    "                                        if \"redirect\" not in sentString:\n",
    "                                            newRow = [line.replace(\"_\",\" \"), destinationPage.replace(\"_\",\" \"), linkTitle, sentString]\n",
    "                                            linkData.append(newRow)\n",
    "    return linkData\n",
    "\n",
    "# Query Wikidata SPARQL Endpoint\n",
    "def checkWikidataForConnections(linkData, repo_name):\n",
    "    print(\"Querying Wikidata\")\n",
    "    filedir = \"data/output/\"\n",
    "    # Modify to query Wikidata directly\n",
    "    #queryHead = f\"http://localhost:7200/repositories/{repo_name}?name=&infer=false&sameAs=false&query=\"\n",
    "    queryHead = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\n",
    "    training_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    queryTail = \"select ?s ?p ?o where {\"\n",
    "    itemsInQuery = {}\n",
    "    batchSize = 20\n",
    "    batchCount = 0\n",
    "    \n",
    "    wp2wd = {}\n",
    "    # load wp2wd map\n",
    "    with open(\"data/wp2wd.txt\", 'r', encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in reader:\n",
    "            wikiString = \"\"\n",
    "            for i in range(len(row) - 1):\n",
    "                if i != 0 and row[i] != row[-1]:\n",
    "                    wikiString += \",\"\n",
    "                wikiString += row[i]\n",
    "            wp2wd[wikiString] = row[-1]\n",
    "    \n",
    "    for i in progressbar.progressbar(range(len(linkData)), redirect_stdout=True):\n",
    "    #for i in range(len(linkData)):\n",
    "        #print(linkData[i])\n",
    "        if queryTail == \"select ?s ?p ?o where {\":\n",
    "            queryTail += \"{\"\n",
    "        else:\n",
    "            queryTail += \"UNION {\"\n",
    "        #subj = \"<https://en.wikipedia.org/wiki/\" + linkData[i][0].replace(\" \",\"_\") + \">\"\n",
    "        #obj = \"<https://en.wikipedia.org/wiki/\" + linkData[i][1].replace(\" \",\"_\") + \">\"\n",
    "        subject_present = True\n",
    "        object_present = True\n",
    "        if linkData[i][0] not in wp2wd:\n",
    "            subject_present = False\n",
    "            print(\"Failed to find Wikidata ID for \", linkData[i][0])\n",
    "        if linkData[i][1] not in wp2wd:\n",
    "            object_present = False\n",
    "            print(\"Failed to find Wikidata ID for \", linkData[i][1])\n",
    "        if (subject_present and object_present):\n",
    "            subj = \"wd:\" + wp2wd[linkData[i][0]]\n",
    "            obj = \"wd:\" + wp2wd[linkData[i][1]]\n",
    "            queryTail += \"VALUES ?s {\"+subj+\"} VALUES ?o {\"+obj+\"} ?s ?p ?o } UNION { VALUES ?s {\"+obj+\"} VALUES ?o {\"+subj+\"} ?s ?p ?o }\"\n",
    "        \n",
    "        linkData[i][0] = linkData[i][0].translate(str.maketrans('', '', string.punctuation)).replace(\"httpsenwikipediaorgwiki\",\"\")\n",
    "        linkData[i][1] = linkData[i][1].translate(str.maketrans('', '', string.punctuation)).replace(\"httpsenwikipediaorgwiki\",\"\")\n",
    "        linkData[i][2] = linkData[i][2].translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        itemsInQuery[subj+obj] = linkData[i]\n",
    "        \n",
    "        if (i % batchSize == 0 and i != 0) or i == len(linkData) - 1:\n",
    "            keysToDelete = set()\n",
    "            queryTail += \"}\"\n",
    "            queryTail = urllib.parse.quote(queryTail)\n",
    "            #print(queryHead + queryTail)\n",
    "            res = str(getWithRetry(queryHead + queryTail).content)\n",
    "            # This is probably the worst error handling possible\n",
    "            if \"Error: 429\" in res or \"414 Request-URI Too Large\" in res:\n",
    "            #if 'Bad Request' in res:\n",
    "                print(res)\n",
    "                raise Exception(\"Server returned error\")\n",
    "            \n",
    "            # WIKIDATA QUERY SERVICE PARSER\n",
    "            elements = []\n",
    "            if \"<result>\" in res:\n",
    "                #print(res)\n",
    "                ressplit = res.split(\"<result>\")\n",
    "                for i in range(1,len(ressplit)):\n",
    "                    urisplit = ressplit[i].split(\"<uri>\")\n",
    "                    for j in range(1,len(urisplit)):\n",
    "                        elements.append(urisplit[j].split(\"</uri>\")[0].replace(\"http://www.wikidata.org/entity/\",\"\").replace(\"http://www.wikidata.org/prop/direct/\",\"\"))\n",
    "            reshape = np.reshape(elements, (-1,3))\n",
    "            \n",
    "            # GRAPH DB API PARSER\n",
    "            #if \"http://www.wikidata.org/prop/direct/\" in res:\n",
    "            #    split_res = str(res[11:-1]).split(\"\\\\r\\\\n\")[:-1]\n",
    "            #    for row in split_res:\n",
    "            #        csv_line = pp.commaSeparatedList.copy().addParseAction(pp.tokenMap(lambda s: s.strip('\"')))\n",
    "            #        row = csv_line.parseString(row).asList()\n",
    "            #        print(row)\n",
    "            #        x = row[1].split(\"http://www.wikidata.org/prop/direct/\")\n",
    "            #        wdp = x[1].split(\"\\\\\")[0]\n",
    "            #        resSubj = \"<\" + row[0].replace(\"\\\\\",\"\") + \">\"\n",
    "            #        resObj = \"<\" + row[2].replace(\"\\\\\",\"\") + \">\"\n",
    "            \n",
    "            for datarow in reshape:\n",
    "                resSubj = \"wd:\"+datarow[0]\n",
    "                resObj = \"wd:\"+datarow[1]\n",
    "                wdp = datarow[2]\n",
    "                if (resSubj+resObj in itemsInQuery):\n",
    "                    thisRow = itemsInQuery[resSubj+resObj]\n",
    "                    training_row = [thisRow[0], thisRow[1], thisRow[2], thisRow[3], wdp, \"Forwards\"]\n",
    "                    #print(training_row)\n",
    "                    training_data.append(training_row)\n",
    "                    keysToDelete.add(resSubj+resObj)\n",
    "                elif (resObj+resSubj in itemsInQuery):\n",
    "                    thisRow = itemsInQuery[resObj+resSubj]\n",
    "                    training_row = [thisRow[0], thisRow[1], thisRow[2], thisRow[3], wdp, \"Backwards\"]\n",
    "                    #print(training_row)\n",
    "                    training_data.append(training_row)\n",
    "                    keysToDelete.add(resObj+resSubj)\n",
    "                else:\n",
    "                    #print(itemsInQuery)\n",
    "                    raise Exception(\"Did not find row from query result in map: \", resSubj, \" \", resObj)\n",
    "            \n",
    "            for key in keysToDelete:\n",
    "                del itemsInQuery[key]\n",
    "            for key in itemsInQuery:\n",
    "                test_data.append(itemsInQuery[key])\n",
    "                \n",
    "            itemsInQuery = {}\n",
    "            queryTail = \"select ?s ?p ?o where {\"\n",
    "            batchCount = batchCount + 1\n",
    "            #print(\"batch \", batchCount, \" complete\")\n",
    "\n",
    "    return training_data, test_data\n",
    "\n",
    "# Neo4j Cypher lookup\n",
    "# DOES NOT Handle page redirects cleanly (as of now)\n",
    "def getStrongLinks(domain):\n",
    "    print(\"Finding strong links for: \", domain)\n",
    "    query = \"MATCH (source:Page {title: \\\"\"+domain+\"\\\"}) - [link1:Link] - (target:Page) - [link2:Link] - (source) WHERE link1 <> link2 RETURN distinct target.title\"\n",
    "    query_data = runWithRetry(query).data()\n",
    "    strong_links = [domain]\n",
    "    for res in query_data:\n",
    "        page = res[\"target.title\"]\n",
    "        strong_links.append(page)\n",
    "    print(\"Found \", str(len(strong_links)), \" strong links\")\n",
    "    return strong_links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b9f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a59a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
