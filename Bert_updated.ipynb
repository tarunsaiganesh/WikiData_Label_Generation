{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"Bert_updated.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IQdCc1iksxFC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623711955079,"user_tz":420,"elapsed":140,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"83391a8b-e95f-4a54-c750-2142b9ddf2e3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"IQdCc1iksxFC","execution_count":43,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bu577Gy6kmrW"},"source":[""],"id":"bu577Gy6kmrW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5X1wnn_hOKj4","tags":[],"executionInfo":{"status":"ok","timestamp":1623711956833,"user_tz":420,"elapsed":165,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n","    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\",\n","    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin\",\n","    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\",\n","    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin\",\n","    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin\",\n","    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin\",\n","    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin\",\n","    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin\",\n","    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin\",\n","    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin\",\n","    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n","    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n","    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin\",\n","}\n","\n","BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n","    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n","    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json\",\n","    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\",\n","    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json\",\n","    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json\",\n","    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json\",\n","    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json\",\n","    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json\",\n","    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json\",\n","    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json\",\n","    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json\",\n","    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json\",\n","    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json\",\n","}\n","\n","BERT_PRETRAINED_VOCAB_ARCHIVE_MAP = {\n","    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n","    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n","    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n","    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n","    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n","    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n","    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n","    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-vocab.txt\",\n","    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-vocab.txt\",\n","    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-vocab.txt\",\n","    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-vocab.txt\",\n","    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt\",\n","    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-vocab.txt\",\n","}"],"id":"5X1wnn_hOKj4","execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAcmGVpxOKj-","tags":[],"executionInfo":{"status":"ok","timestamp":1623711959306,"user_tz":420,"elapsed":135,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["import numpy as np\n","import torch\n","import os\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'"],"id":"NAcmGVpxOKj-","execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"qXBMq9ywOorW","jupyter":{"outputs_hidden":true,"source_hidden":true},"tags":[],"executionInfo":{"status":"ok","timestamp":1623711963001,"user_tz":420,"elapsed":2746,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"c7056071-972b-4cd9-b01b-11bbdba4a951"},"source":["!pip install transformers"],"id":"qXBMq9ywOorW","execution_count":46,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"mSXhYvCRO-D1","jupyter":{"outputs_hidden":true,"source_hidden":true},"tags":[],"executionInfo":{"status":"ok","timestamp":1623711965493,"user_tz":420,"elapsed":2496,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"28cf7245-c10f-4ccc-e9b8-cba3ac25972a"},"source":["!pip install pytorch-pretrained-bert"],"id":"mSXhYvCRO-D1","execution_count":47,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.94)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n","Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.4.2)\n","Requirement already satisfied: botocore<1.21.0,>=1.20.94 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.94)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.94->boto3->pytorch-pretrained-bert) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.94->boto3->pytorch-pretrained-bert) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uht9JyJgOKkA","executionInfo":{"status":"ok","timestamp":1623713394825,"user_tz":420,"elapsed":135,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["import pandas as pd\n","from transformers import BertTokenizer\n","from transformers import BertConfig\n","from transformers import BertForSequenceClassification\n","from transformers import BertModel\n","\n","from pytorch_pretrained_bert import BertAdam\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"id":"Uht9JyJgOKkA","execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzwAcChEOKkB","executionInfo":{"status":"ok","timestamp":1623713433735,"user_tz":420,"elapsed":5272,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"0c1afe7a-8f9c-4c1c-da2a-517968e4a9df"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# bert_config='torch_bert_pretrained/bert-base-uncased/bert_config.json'#配置文件\n","\n","\n","df = pd.read_csv('/content/drive/Shared drives/CS272/data/extended_datasets/roman_history_training_reduced_labels.csv')\n","print('data size:', len(df))\n","\n","#sentence process\n","sentencses=['[CLS] ' + str(sent) + ' [SEP]' for sent in df['Sentence Text'].values]\n","labels=df['Wikidata Property Label'].values\n","print(\"The first sentence: \",sentencses[0],'\\n')\n","\n","origin_titles = df['Origin Page'].values\n","destination_titles = df['Destination Page'].values\n","origin_encoder = LabelEncoder()\n","origin_encoded = origin_encoder.fit_transform(origin_titles)\n","destination_encoder = LabelEncoder()\n","destination_encoded = destination_encoder.fit_transform(destination_titles)\n","print('Origin encoded: ', origin_encoded[0])\n","print('Destination encoded: ', destination_encoded[0])\n","\n","tokenizer=BertTokenizer.from_pretrained(BERT_PRETRAINED_VOCAB_ARCHIVE_MAP['bert-base-uncased'],do_lower_case=True)\n","tokenized_sents=[tokenizer.tokenize(sent) for sent in sentencses]\n","print(\"The first sentence of tokenized: \",tokenized_sents[0],'\\n')\n","\n","# Max length of sentence\n","MAX_LEN=32\n","\n","# word to idx\n","input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_sents]\n","# Add origin and destination features\n","for i in range(len(input_ids)):\n","    input_ids[i].insert(0,destination_encoded[i])\n","    input_ids[i].insert(0,origin_encoded[i])\n","\n","print(\"After Tokenized:\",input_ids[0],'\\n')\n","\n","#做PADDING,这里使用keras的包做pad，也可以自己手动做pad,truncating表示大于最大长度截断\n","#大于128做截断，小于128做PADDING\n","\"\"\"\n","If the sentence length is larger than 64, we cut it \n","If the sentence length is shorted than 64, we use padding\n","\"\"\"\n","input_ids=pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","print(\"First Sentence of Padding:\",input_ids[0],'\\n')\n","\n","# create mask\n","attention_masks = []\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","print(\"First Sentence of Attention mask:\",attention_masks[0])\n"],"id":"GzwAcChEOKkB","execution_count":72,"outputs":[{"output_type":"stream","text":["data size: 6276\n","The first sentence:  [CLS] United States–Holy See relations are bilateral relations between the United States and the Holy See [SEP] \n","\n","Origin encoded:  1791\n","Destination encoded:  400\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1631: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["The first sentence of tokenized:  ['[CLS]', 'united', 'states', '–', 'holy', 'see', 'relations', 'are', 'bilateral', 'relations', 'between', 'the', 'united', 'states', 'and', 'the', 'holy', 'see', '[SEP]'] \n","\n","After Tokenized: [1791, 400, 101, 2142, 2163, 1516, 4151, 2156, 4262, 2024, 17758, 4262, 2090, 1996, 2142, 2163, 1998, 1996, 4151, 2156, 102] \n","\n","First Sentence of Padding: [ 1791   400   101  2142  2163  1516  4151  2156  4262  2024 17758  4262\n","  2090  1996  2142  2163  1998  1996  4151  2156   102     0     0     0\n","     0     0     0     0     0     0     0     0] \n","\n","First Sentence of Attention mask: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ph_UvtvaOKkE","executionInfo":{"status":"ok","timestamp":1623713435894,"user_tz":420,"elapsed":143,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"4c53bad3-dfa4-47c2-ad17-1d564a567954"},"source":["#split train and dev\n","from sklearn.model_selection import train_test_split\n","\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n","                                                            random_state=2018, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.1)\n","\n","print(len(input_ids))\n","print(len(train_inputs),len(validation_inputs))\n","print(\"First inputs in train data\",train_inputs[0])\n","print(\"Firstr mask in train data\",train_masks[0])\n"],"id":"Ph_UvtvaOKkE","execution_count":73,"outputs":[{"output_type":"stream","text":["6276\n","5648 628\n","First inputs in train data [  990   317   101 24707 26210  4244  3540  4904  2413 15498 16048  2692\n","  2620 11387  2509  2243 29679  3207 25353 29694  1048 29275 21590  6719\n"," 24707  2006  1996  9686  3540  4904 27263  4232]\n","Firstr mask in train data [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"he4HKNSrOKkG","executionInfo":{"status":"ok","timestamp":1623713438293,"user_tz":420,"elapsed":132,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"b2b3f91a-84be-4cc2-b863-7914c0e5e6d4"},"source":["import numpy as np\n","from numpy import array\n","from numpy import argmax\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","\"\"\"\n","Change the string label into the integer label by usig oneHot\n","\n","In this case, the number of labels is 10\n","\n","\"\"\"\n","\n","def oneHot(data):\n","   \n","    values = array(data)\n","#     print(values)\n","    # integer encode\n","    label_encoder = LabelEncoder()\n","    integer_encoded = label_encoder.fit_transform(values)\n","#     print(integer_encoded)\n","\n","\n","    # one hot encode\n","    encoded = to_categorical(integer_encoded)\n","    inverted = argmax(encoded[0])\n","    print('Result:', integer_encoded)\n","    maxLabel = np.max(integer_encoded)\n","    return label_encoder, maxLabel\n","\n","combo_list = train_labels.copy().tolist()\n","combo_list.extend(validation_labels)\n","label_encoder, no_of_classes = oneHot(combo_list)\n","tr_label = label_encoder.transform(train_labels)\n","va_label = label_encoder.transform(validation_labels)\n","# len(set(tr_label))\n","print('train label:', (tr_label))\n","print(label_encoder.inverse_transform(tr_label))\n","print(no_of_classes)"],"id":"he4HKNSrOKkG","execution_count":74,"outputs":[{"output_type":"stream","text":["Result: [1 1 0 ... 1 8 4]\n","train label: [1 1 0 ... 1 1 1]\n","['P17' 'P17' 'P131' ... 'P17' 'P17' 'P17']\n","9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e3OTEAfnOKkI","executionInfo":{"status":"ok","timestamp":1623713440380,"user_tz":420,"elapsed":136,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}}},"source":["#change into tensor format\n","import torch\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import RandomSampler\n","from torch.utils.data import DataLoader\n","from torch.utils.data import SequentialSampler\n","\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(tr_label)\n","validation_labels = torch.tensor(va_label)\n","# train_labels = torch.tensor(tr_label).reshape(train_inputs.shape[0],1)\n","# validation_labels = torch.tensor(va_label).reshape(validation_inputs.shape[0],1)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)\n","\n","\n","batch_size = 16\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"id":"e3OTEAfnOKkI","execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDZ3gGKCOKkI","executionInfo":{"status":"ok","timestamp":1623713442814,"user_tz":420,"elapsed":128,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"6cbe375f-7c2c-4af8-8e02-670d181a55f7"},"source":["train_inputs.shape[0]"],"id":"pDZ3gGKCOKkI","execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5648"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_r5QP0B1OKkK","executionInfo":{"status":"ok","timestamp":1623713445556,"user_tz":420,"elapsed":1959,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"2ef80f3b-9f3e-454b-d477-3c2ceb6e1d60"},"source":["modelConfig = BertConfig.from_pretrained(BERT_PRETRAINED_CONFIG_ARCHIVE_MAP['bert-base-uncased'],num_labels=no_of_classes+1)\n","# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n","#     num_labels = 10)\n","\n","\n","# modelConfig = BertConfig.from_pretrained('bert-base-uncased/bert_config.json')\n","model = BertForSequenceClassification.from_pretrained(BERT_PRETRAINED_MODEL_ARCHIVE_MAP['bert-base-uncased'],\n","                                                      config=modelConfig)\n","\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]\n","\n","optimizer = BertAdam(model.parameters(), #optimizer_grouped_parameters,\n","                     lr=2e-5,\n","                     warmup=.1)\n"],"id":"_r5QP0B1OKkK","execution_count":77,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","t_total value of -1 results in schedule not being applied\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_m-DIW_OKkX","executionInfo":{"status":"ok","timestamp":1623713447192,"user_tz":420,"elapsed":156,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"03a18b6c-8672-401f-81b7-dd22935a5601"},"source":["params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"id":"W_m-DIW_OKkX","execution_count":78,"outputs":[{"output_type":"stream","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                          (10, 768)\n","classifier.bias                                                (10,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"voc9G71nOKkY","executionInfo":{"status":"ok","timestamp":1623714113699,"user_tz":420,"elapsed":659078,"user":{"displayName":"Tarun Sai Ganesh Nerella","photoUrl":"","userId":"08167125171692557135"}},"outputId":"7095134c-67e6-4656-e9ba-dc526265bf5e"},"source":["#定义一个计算准确率的函数\n","from tqdm import trange \n","\n","\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","  \n","def per_label_accuracy(preds, labels, per_label_acc, label_count):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    for p, l in zip(pred_flat, labels_flat):\n","      if l not in set(per_label_acc):\n","        per_label_acc[l] = 0\n","      if l not in set(label_count):\n","        label_count[l] = 0\n","      \n","      label_count[l] += 1\n","      if p==l:\n","        per_label_acc[l] += 1\n","      else:\n","        per_label_acc[l] += 0\n","    \n","\n","#训练开始\n","train_loss_set = []#可以将loss加入到列表中，后期画图使用\n","epochs = 10\n","device = 'cuda'\n","model = model.to(device)\n","for _ in trange(epochs, desc=\"Epoch\"):\n","    #训练开始\n","    model.train()\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    for step, batch in enumerate(train_dataloader):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","#         print(b_labels)\n","        optimizer.zero_grad()\n","        #取第一个位置，BertForSequenceClassification第一个位置是Loss，第二个位置是[CLS]的logits\n","        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n","        train_loss_set.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","        tr_loss += loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n","    #模型评估\n","    model.eval()\n","    per_label_acc, label_count = {}, {}\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    for batch in validation_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to(device).cpu().numpy()\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        per_label_accuracy(logits, label_ids, per_label_acc, label_count)   ##\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","    print(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))\n","    for k in set(label_count):\n","      print(\"{} = {} Per Label Validation Accuracy: {}\".format(k, label_encoder.inverse_transform(np.array([k])), per_label_acc[k] / label_count[k]))"],"id":"voc9G71nOKkY","execution_count":79,"outputs":[{"output_type":"stream","text":["\n","\n","Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.292600570251854\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  10%|█         | 1/10 [01:05<09:53, 65.98s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6484375\n","0 = ['P131'] Per Label Validation Accuracy: 0.015873015873015872\n","1 = ['P17'] Per Label Validation Accuracy: 0.9769230769230769\n","2 = ['P19'] Per Label Validation Accuracy: 0.23529411764705882\n","3 = ['P20'] Per Label Validation Accuracy: 0.3870967741935484\n","4 = ['P27'] Per Label Validation Accuracy: 0.625\n","5 = ['P276'] Per Label Validation Accuracy: 0.125\n","6 = ['P361'] Per Label Validation Accuracy: 0.24242424242424243\n","7 = ['P39'] Per Label Validation Accuracy: 0.717948717948718\n","8 = ['P47'] Per Label Validation Accuracy: 0.7391304347826086\n","9 = ['P710'] Per Label Validation Accuracy: 0.7692307692307693\n","Train loss: 0.8722009159856747\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  20%|██        | 2/10 [02:11<08:46, 65.82s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.721875\n","0 = ['P131'] Per Label Validation Accuracy: 0.36507936507936506\n","1 = ['P17'] Per Label Validation Accuracy: 0.9461538461538461\n","2 = ['P19'] Per Label Validation Accuracy: 0.029411764705882353\n","3 = ['P20'] Per Label Validation Accuracy: 0.8064516129032258\n","4 = ['P27'] Per Label Validation Accuracy: 0.78125\n","5 = ['P276'] Per Label Validation Accuracy: 0.1875\n","6 = ['P361'] Per Label Validation Accuracy: 0.42424242424242425\n","7 = ['P39'] Per Label Validation Accuracy: 0.9487179487179487\n","8 = ['P47'] Per Label Validation Accuracy: 0.782608695652174\n","9 = ['P710'] Per Label Validation Accuracy: 0.6923076923076923\n","Train loss: 0.5560240550520738\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  30%|███       | 3/10 [03:17<07:41, 65.92s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.790625\n","0 = ['P131'] Per Label Validation Accuracy: 0.6349206349206349\n","1 = ['P17'] Per Label Validation Accuracy: 0.95\n","2 = ['P19'] Per Label Validation Accuracy: 0.2647058823529412\n","3 = ['P20'] Per Label Validation Accuracy: 0.7096774193548387\n","4 = ['P27'] Per Label Validation Accuracy: 0.875\n","5 = ['P276'] Per Label Validation Accuracy: 0.3125\n","6 = ['P361'] Per Label Validation Accuracy: 0.5454545454545454\n","7 = ['P39'] Per Label Validation Accuracy: 0.9743589743589743\n","8 = ['P47'] Per Label Validation Accuracy: 0.8695652173913043\n","9 = ['P710'] Per Label Validation Accuracy: 0.6538461538461539\n","Train loss: 0.39107829939162425\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  40%|████      | 4/10 [04:23<06:35, 65.97s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7796875\n","0 = ['P131'] Per Label Validation Accuracy: 0.7777777777777778\n","1 = ['P17'] Per Label Validation Accuracy: 0.9153846153846154\n","2 = ['P19'] Per Label Validation Accuracy: 0.11764705882352941\n","3 = ['P20'] Per Label Validation Accuracy: 0.8387096774193549\n","4 = ['P27'] Per Label Validation Accuracy: 0.859375\n","5 = ['P276'] Per Label Validation Accuracy: 0.375\n","6 = ['P361'] Per Label Validation Accuracy: 0.5454545454545454\n","7 = ['P39'] Per Label Validation Accuracy: 0.9743589743589743\n","8 = ['P47'] Per Label Validation Accuracy: 0.7608695652173914\n","9 = ['P710'] Per Label Validation Accuracy: 0.6923076923076923\n","Train loss: 0.29912274473323025\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  50%|█████     | 5/10 [05:30<05:30, 66.10s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7796875\n","0 = ['P131'] Per Label Validation Accuracy: 0.7142857142857143\n","1 = ['P17'] Per Label Validation Accuracy: 0.95\n","2 = ['P19'] Per Label Validation Accuracy: 0.2647058823529412\n","3 = ['P20'] Per Label Validation Accuracy: 0.5161290322580645\n","4 = ['P27'] Per Label Validation Accuracy: 0.875\n","5 = ['P276'] Per Label Validation Accuracy: 0.3125\n","6 = ['P361'] Per Label Validation Accuracy: 0.5151515151515151\n","7 = ['P39'] Per Label Validation Accuracy: 0.8717948717948718\n","8 = ['P47'] Per Label Validation Accuracy: 0.7608695652173914\n","9 = ['P710'] Per Label Validation Accuracy: 0.8076923076923077\n","Train loss: 0.25548451733656713\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  60%|██████    | 6/10 [06:36<04:24, 66.11s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7890625\n","0 = ['P131'] Per Label Validation Accuracy: 0.7142857142857143\n","1 = ['P17'] Per Label Validation Accuracy: 0.9230769230769231\n","2 = ['P19'] Per Label Validation Accuracy: 0.23529411764705882\n","3 = ['P20'] Per Label Validation Accuracy: 0.7096774193548387\n","4 = ['P27'] Per Label Validation Accuracy: 0.78125\n","5 = ['P276'] Per Label Validation Accuracy: 0.375\n","6 = ['P361'] Per Label Validation Accuracy: 0.5757575757575758\n","7 = ['P39'] Per Label Validation Accuracy: 0.9743589743589743\n","8 = ['P47'] Per Label Validation Accuracy: 0.9347826086956522\n","9 = ['P710'] Per Label Validation Accuracy: 0.7307692307692307\n","Train loss: 0.22796066831164166\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  70%|███████   | 7/10 [07:42<03:18, 66.10s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.76875\n","0 = ['P131'] Per Label Validation Accuracy: 0.746031746031746\n","1 = ['P17'] Per Label Validation Accuracy: 0.8653846153846154\n","2 = ['P19'] Per Label Validation Accuracy: 0.17647058823529413\n","3 = ['P20'] Per Label Validation Accuracy: 0.6129032258064516\n","4 = ['P27'] Per Label Validation Accuracy: 0.75\n","5 = ['P276'] Per Label Validation Accuracy: 0.375\n","6 = ['P361'] Per Label Validation Accuracy: 0.6060606060606061\n","7 = ['P39'] Per Label Validation Accuracy: 0.9743589743589743\n","8 = ['P47'] Per Label Validation Accuracy: 0.9565217391304348\n","9 = ['P710'] Per Label Validation Accuracy: 0.8076923076923077\n","Train loss: 0.2013489229699581\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  80%|████████  | 8/10 [08:48<02:12, 66.06s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.790625\n","0 = ['P131'] Per Label Validation Accuracy: 0.7142857142857143\n","1 = ['P17'] Per Label Validation Accuracy: 0.9346153846153846\n","2 = ['P19'] Per Label Validation Accuracy: 0.20588235294117646\n","3 = ['P20'] Per Label Validation Accuracy: 0.6451612903225806\n","4 = ['P27'] Per Label Validation Accuracy: 0.75\n","5 = ['P276'] Per Label Validation Accuracy: 0.40625\n","6 = ['P361'] Per Label Validation Accuracy: 0.5151515151515151\n","7 = ['P39'] Per Label Validation Accuracy: 0.9743589743589743\n","8 = ['P47'] Per Label Validation Accuracy: 0.9565217391304348\n","9 = ['P710'] Per Label Validation Accuracy: 0.7307692307692307\n","Train loss: 0.1828353228931323\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch:  90%|█████████ | 9/10 [09:53<01:05, 65.81s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.784375\n","0 = ['P131'] Per Label Validation Accuracy: 0.7142857142857143\n","1 = ['P17'] Per Label Validation Accuracy: 0.9230769230769231\n","2 = ['P19'] Per Label Validation Accuracy: 0.29411764705882354\n","3 = ['P20'] Per Label Validation Accuracy: 0.4838709677419355\n","4 = ['P27'] Per Label Validation Accuracy: 0.71875\n","5 = ['P276'] Per Label Validation Accuracy: 0.40625\n","6 = ['P361'] Per Label Validation Accuracy: 0.5454545454545454\n","7 = ['P39'] Per Label Validation Accuracy: 0.9743589743589743\n","8 = ['P47'] Per Label Validation Accuracy: 1.0\n","9 = ['P710'] Per Label Validation Accuracy: 0.7307692307692307\n","Train loss: 0.1698262271268172\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Epoch: 100%|██████████| 10/10 [10:58<00:00, 65.87s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.778125\n","0 = ['P131'] Per Label Validation Accuracy: 0.6984126984126984\n","1 = ['P17'] Per Label Validation Accuracy: 0.926923076923077\n","2 = ['P19'] Per Label Validation Accuracy: 0.2647058823529412\n","3 = ['P20'] Per Label Validation Accuracy: 0.41935483870967744\n","4 = ['P27'] Per Label Validation Accuracy: 0.75\n","5 = ['P276'] Per Label Validation Accuracy: 0.34375\n","6 = ['P361'] Per Label Validation Accuracy: 0.5757575757575758\n","7 = ['P39'] Per Label Validation Accuracy: 0.9743589743589743\n","8 = ['P47'] Per Label Validation Accuracy: 0.9565217391304348\n","9 = ['P710'] Per Label Validation Accuracy: 0.7307692307692307\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjFnSAcwFj72","executionInfo":{"status":"ok","timestamp":1623508695783,"user_tz":-480,"elapsed":35344,"user":{"displayName":"Qingyu Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7galQfyO8ViOu78YFt0oVgz9kbR37jqwgHhhP=s64","userId":"09594612251268525839"}},"outputId":"9c324d14-ed17-471a-8572-cb2d71aedec0"},"source":["\"\"\"\n","Test combine LabelEncoder performance\n","\n","\"\"\"\n","\n","\n","# ###################################################################################################\n","\n","# combine train and test data into one LabelEncoder function\n","\n","train_df = pd.read_csv('/content/drive/Shared drives/CS272/data/extended_datasets/roman_history_training_dedup.csv')\n","test_df = pd.read_csv('/content/drive/Shared drives/CS272/data/extended_datasets/roman_history_test.csv')\n","\n","sentence_text = pd.concat([train_df['Sentence Text'], test_df['Sentence Text']],axis=0).values\n","\n","\n","combine_ori_titles = pd.concat([train_df['Origin Page'], test_df['Origin Page']],axis=0).values\n","\n","combine_des_titles = pd.concat([train_df['Destination Page'], test_df['Destination Page']],axis=0).values\n","\n","combine_ori_encoder = LabelEncoder()\n","combine_des_encoder = LabelEncoder()\n","\n","\n","\n","combine_ori_encoded = combine_ori_encoder.fit_transform(combine_ori_titles)\n","combine_des_encoded = combine_des_encoder.fit_transform(combine_des_titles)\n","\n","sentencses=['[CLS] ' + str(sent) + ' [SEP]' for sent in sentence_text]\n","\n","\n","print('Origin encoded: ', combine_ori_encoded[0])\n","print('Destination encoded: ', combine_des_encoded[0])\n","\n","tokenizer=BertTokenizer.from_pretrained(BERT_PRETRAINED_VOCAB_ARCHIVE_MAP['bert-base-uncased'],do_lower_case=True)\n","tokenized_sents=[tokenizer.tokenize(sent) for sent in sentencses]\n","print(\"The first sentence of tokenized: \",tokenized_sents[0],'\\n')\n","\n","# Max length of sentence\n","MAX_LEN=32\n","\n","# word to idx\n","combine_input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_sents] ################## input_ids definition\n","print('Input ids size:', len(input_ids),'\\n')\n","# Add origin and destination features\n","for i in range(len(combine_input_ids)):\n","    combine_input_ids[i].insert(0,combine_ori_encoded[i])\n","    combine_input_ids[i].insert(0,combine_des_encoded[i])\n","\n","print(\"After Tokenized:\",combine_input_ids[0],'\\n')\n","\n","#做PADDING,这里使用keras的包做pad，也可以自己手动做pad,truncating表示大于最大长度截断\n","#大于128做截断，小于128做PADDING\n","\"\"\"\n","If the sentence length is larger than 64, we cut it \n","If the sentence length is shorted than 64, we use padding\n","\"\"\"\n","combine_input_ids=pad_sequences(combine_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","print(\"First Sentence of Padding:\",input_ids[0],'\\n')\n","\n","# create mask\n","attention_masks = []\n","for seq in combine_input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","print(\"First Sentence of Attention mask:\",attention_masks[0])\n","\n","\n","\n","\n","# test_origin_titles = test_df['Origin Page'].values\n","# test_destination_titles = test_df['Destination Page'].values\n","# test_origin_encoded = combine_ori_encoder.transform(test_origin_titles)\n","# test_destination_encoded = combine_des_encoder.transform(test_destination_titles)\n","\n","\n","# print('encoer length',len(test_destination_encoded))\n","\n","\n","\n","# ###################################################################################################"],"id":"rjFnSAcwFj72","execution_count":null,"outputs":[{"output_type":"stream","text":["Origin encoded:  4625\n","Destination encoded:  1848\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1631: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["The first sentence of tokenized:  ['[CLS]', 'united', 'states', '–', 'holy', 'see', 'relations', 'are', 'bilateral', 'relations', 'between', 'the', 'united', 'states', 'and', 'the', 'holy', 'see', '[SEP]'] \n","\n","Input ids size: 12349 \n","\n","After Tokenized: [1848, 4625, 101, 2142, 2163, 1516, 4151, 2156, 4262, 2024, 17758, 4262, 2090, 1996, 2142, 2163, 1998, 1996, 4151, 2156, 102] \n","\n","First Sentence of Padding: [ 2400   772   101  2142  2163  1516  4151  2156  4262  2024 17758  4262\n","  2090  1996  2142  2163  1998  1996  4151  2156   102     0     0     0\n","     0     0     0     0     0     0     0     0] \n","\n","First Sentence of Attention mask: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KLegwCnsXP8v"},"source":[""],"id":"KLegwCnsXP8v","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hmM5ttdNHjaa","executionInfo":{"status":"ok","timestamp":1623509084492,"user_tz":-480,"elapsed":381525,"user":{"displayName":"Qingyu Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7galQfyO8ViOu78YFt0oVgz9kbR37jqwgHhhP=s64","userId":"09594612251268525839"}},"outputId":"52e80402-d13e-4a44-8712-d5fba3035853"},"source":["\"\"\"\n","Test combine LabelEncoder performance\n","\n","\"\"\"\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","\n","test_df = pd.read_csv('/content/drive/Shared drives/CS272/data/extended_datasets/roman_history_test.csv')\n","print('data size:', len(test_df))\n","\n","#sentence process\n","sents=['[CLS] ' + str(sent) + ' [SEP]' for sent in test_df['Sentence Text'].values]\n","print(\"The first sentence: \",sents[0],'\\n')\n","\n","test_origin_titles = test_df['Origin Page'].values\n","test_destination_titles = test_df['Destination Page'].values\n","test_origin_encoded = combine_ori_encoder.transform(test_origin_titles)\n","test_destination_encoded = combine_des_encoder.transform(test_destination_titles)\n","\n","print('Test Origin encoded: ', test_origin_encoded[0])\n","print('Test Destination encoded: ', test_destination_encoded[0])\n","\n","test_tokenized_sents=[tokenizer.tokenize(sent) for sent in sents]\n","print(\"The first sentence of tokenized: \",test_tokenized_sents[0],'\\n')\n","\n","test_input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in test_tokenized_sents]\n","# Add origin and destination features\n","\n","print('length of ids',len(test_input_ids))\n","# for i in range(len(test_input_ids)):\n","#     # print('ID: ',i, len(input_ids))\n","#     combine_input_ids[i].tolist().insert(0,test_destination_encoded[i])\n","#     combine_input_ids[i].tolist().insert(0,test_origin_encoded[i])\n","    \n","print(\"After Tokenized:\",test_input_ids[0],'\\n')\n","\n","#做PADDING,这里使用keras的包做pad，也可以自己手动做pad,truncating表示大于最大长度截断\n","#大于128做截断，小于128做PADDING\n","\"\"\"\n","If the sentence length is larger than 64, we cut it \n","If the sentence length is shorted than 64, we use padding\n","\"\"\"\n","test_input_ids=pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","print(\"First Sentence of Padding:\",test_input_ids[0],'\\n')\n","\n","# create mask\n","test_attention_masks = []\n","for seq in test_input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    test_attention_masks.append(seq_mask)\n","print(\"First Sentence of Attention mask:\",test_attention_masks[0])\n","\n","test_input_ids = torch.tensor(test_input_ids)\n","test_attention_masks = torch.tensor(test_attention_masks)\n","\n","test_input_ids = TensorDataset(test_input_ids, test_attention_masks)\n","test_sampler = SequentialSampler(test_input_ids)\n","test_dataloader = DataLoader(test_input_ids, sampler=test_sampler, batch_size=1)\n","\n","model.eval()\n","test_result = pd.DataFrame()\n","id_list, wiki_label_list = [],[]\n","# with open('test_labels.csv', 'w') as f:\n","for batch in test_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask = batch\n","    with torch.no_grad():\n","        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","    logits = logits.detach().cpu().numpy()\n","    test_labels = label_encoder.inverse_transform(np.argmax(logits, axis=1))\n","    id_list.append(np.argmax(logits, axis=1)[0])\n","    wiki_label_list.append(test_labels[0])\n","\n","    # print('Test labels: ', test_labels)\n","    # print('ID:', np.argmax(logits, axis=1))\n","\n","test_result['Encoder Labels'] = id_list\n","test_result['Wikidata Labels'] = wiki_label_list\n","      # f.write(str(test_labels[0])+'\\n')\n","test_result.to_csv('/content/drive/Shared drives/CS272/data/bert_test_results/roman_history_dedup_result.csv',index=False)\n","    # break\n","# f.close()\n"],"id":"hmM5ttdNHjaa","execution_count":null,"outputs":[{"output_type":"stream","text":["data size: 37698\n","The first sentence:  [CLS] The Roman Emperors Route Latin Itinerarium Romanum Serbiae Serbian Путевима римских императора is a tourism and archaeology project in Serbia a route spanning 600160km with several ancient Roman sites among which are notable cities estates and birthplaces see Roman heritage in Serbia [SEP] \n","\n","Test Origin encoded:  8437\n","Test Destination encoded:  3517\n","The first sentence of tokenized:  ['[CLS]', 'the', 'roman', 'emperors', 'route', 'latin', 'it', '##iner', '##arium', 'roman', '##um', 'serbia', '##e', 'serbian', 'п', '##у', '##т', '##е', '##в', '##и', '##м', '##а', 'р', '##и', '##м', '##с', '##к', '##и', '##х', 'и', '##м', '##п', '##е', '##р', '##а', '##т', '##о', '##р', '##а', 'is', 'a', 'tourism', 'and', 'archaeology', 'project', 'in', 'serbia', 'a', 'route', 'spanning', '600', '##16', '##0', '##km', 'with', 'several', 'ancient', 'roman', 'sites', 'among', 'which', 'are', 'notable', 'cities', 'estates', 'and', 'birthplace', '##s', 'see', 'roman', 'heritage', 'in', 'serbia', '[SEP]'] \n","\n","length of ids 37698\n","After Tokenized: [101, 1996, 3142, 19655, 2799, 3763, 2009, 26455, 17285, 3142, 2819, 7238, 2063, 6514, 1194, 29748, 22919, 15290, 25529, 10325, 29745, 10260, 1195, 10325, 29745, 29747, 23925, 10325, 29750, 1188, 29745, 29746, 15290, 16856, 10260, 22919, 14150, 16856, 10260, 2003, 1037, 6813, 1998, 12009, 2622, 1999, 7238, 1037, 2799, 13912, 5174, 16048, 2692, 22287, 2007, 2195, 3418, 3142, 4573, 2426, 2029, 2024, 3862, 3655, 8707, 1998, 14508, 2015, 2156, 3142, 4348, 1999, 7238, 102] \n","\n","First Sentence of Padding: [  101  1996  3142 19655  2799  3763  2009 26455 17285  3142  2819  7238\n","  2063  6514  1194 29748 22919 15290 25529 10325 29745 10260  1195 10325\n"," 29745 29747 23925 10325 29750  1188 29745 29746] \n","\n","First Sentence of Attention mask: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aGBxvkdVlGLV","executionInfo":{"status":"ok","timestamp":1623475826347,"user_tz":-480,"elapsed":316,"user":{"displayName":"Qingyu Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7galQfyO8ViOu78YFt0oVgz9kbR37jqwgHhhP=s64","userId":"09594612251268525839"}},"outputId":"1bd6b2ad-f1fd-4e19-d934-cb7090de1802"},"source":["# roman reduced\n","print(label_encoder.classes_)"],"id":"aGBxvkdVlGLV","execution_count":null,"outputs":[{"output_type":"stream","text":["['P131' 'P17' 'P19' 'P20' 'P27' 'P276' 'P361' 'P39' 'P47' 'P710']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_V8GQM7PvjJX","executionInfo":{"status":"ok","timestamp":1623476575301,"user_tz":-480,"elapsed":605,"user":{"displayName":"Qingyu Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7galQfyO8ViOu78YFt0oVgz9kbR37jqwgHhhP=s64","userId":"09594612251268525839"}},"outputId":"60086fde-2d44-47aa-8ac4-69d3dcf6e9cd"},"source":["# sustainability reduced\n","print(label_encoder.classes_)\n","\n"],"id":"_V8GQM7PvjJX","execution_count":null,"outputs":[{"output_type":"stream","text":["['P131' 'P150' 'P17' 'P27' 'P279' 'P361' 'P463' 'P47' 'P527' 'P530']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lX2MOGUCOYfL","executionInfo":{"status":"ok","timestamp":1623419410683,"user_tz":-480,"elapsed":320,"user":{"displayName":"Qingyu Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7galQfyO8ViOu78YFt0oVgz9kbR37jqwgHhhP=s64","userId":"09594612251268525839"}},"outputId":"cf9b57e7-57d3-40b5-9070-f3668042a102"},"source":["test_labels = pd.read_csv('/content/test_labels.csv')\n","test_labels.value_counts()"],"id":"lX2MOGUCOYfL","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["P279\n","P279    4915\n","P17     3556\n","P361    2638\n","P27     1195\n","P463     767\n","P131     521\n","P527     372\n","P530     191\n","P47      116\n","P150      56\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"aJIxcWjTqTsC","colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"status":"error","timestamp":1623415464111,"user_tz":-480,"elapsed":10206,"user":{"displayName":"Qingyu Song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7galQfyO8ViOu78YFt0oVgz9kbR37jqwgHhhP=s64","userId":"09594612251268525839"}},"outputId":"f63b4f0f-f333-4656-9063-e0793d934ec9"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","\n","\n","test_df = pd.read_csv('/content/drive/Shared drives/CS272/data/extended_datasets/sustainability_results_test.csv')\n","print('data size:', len(test_df))\n","\n","#sentence process\n","sents=['[CLS] ' + sent + ' [SEP]' for sent in test_df['Sentence Text'].values]\n","print(\"The first sentence: \",sents[0],'\\n')\n","\n","test_origin_titles = test_df['Origin Page'].values\n","test_destination_titles = test_df['Destination Page'].values\n","test_origin_encoded = origin_encoder.transform(test_origin_titles)\n","test_destination_encoded = destination_encoder.transform(test_destination_titles)\n","\n","\n","\n","\n","print('Test Origin encoded: ', test_origin_encoded[0])\n","print('Test Destination encoded: ', test_destination_encoded[0])\n","\n","test_tokenized_sents=[tokenizer.tokenize(sent) for sent in sents]\n","print(\"The first sentence of tokenized: \",test_tokenized_sents[0],'\\n')\n","\n","test_input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in test_tokenized_sents]\n","# Add origin and destination features\n","for i in range(len(test_input_ids)):\n","    input_ids[i].tolist().insert(0,test_destination_encoded[i])\n","    input_ids[i].tolist().insert(0,test_origin_encoded[i])\n","\n","print(\"After Tokenized:\",test_input_ids[0],'\\n')\n","\n","#做PADDING,这里使用keras的包做pad，也可以自己手动做pad,truncating表示大于最大长度截断\n","#大于128做截断，小于128做PADDING\n","\"\"\"\n","If the sentence length is larger than 64, we cut it \n","If the sentence length is shorted than 64, we use padding\n","\"\"\"\n","test_input_ids=pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","print(\"First Sentence of Padding:\",test_input_ids[0],'\\n')\n","\n","# create mask\n","test_attention_masks = []\n","for seq in test_input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    test_attention_masks.append(seq_mask)\n","print(\"First Sentence of Attention mask:\",test_attention_masks[0])\n","\n","test_input_ids = torch.tensor(test_input_ids)\n","test_attention_masks = torch.tensor(test_attention_masks)\n","\n","test_input_ids = TensorDataset(test_input_ids, test_attention_masks)\n","test_sampler = SequentialSampler(test_input_ids)\n","test_dataloader = DataLoader(test_input_ids, sampler=test_sampler, batch_size=1)\n","\n","model.eval()\n","with open('test_labels.csv', 'w') as f:\n","  for batch in test_dataloader:\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_input_mask = batch\n","      with torch.no_grad():\n","          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","      logits = logits.detach().cpu().numpy()\n","      test_labels = label_encoder.inverse_transform(np.argmax(logits, axis=1))\n","      # print('Test labels: ', test_labels)\n","      f.write(str(test_labels[0])+'\\n')\n","f.close()\n"],"id":"aJIxcWjTqTsC","execution_count":null,"outputs":[{"output_type":"stream","text":["data size: 14328\n","The first sentence:  [CLS] Drinking water also known as potable water is water that is safe to drink or use for food preparation [SEP] \n","\n","encoer length 14328\n","Test Origin encoded:  1105\n","Test Destination encoded:  1858\n","The first sentence of tokenized:  ['[CLS]', 'drinking', 'water', 'also', 'known', 'as', 'pot', '##able', 'water', 'is', 'water', 'that', 'is', 'safe', 'to', 'drink', 'or', 'use', 'for', 'food', 'preparation', '[SEP]'] \n","\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-d5a88d9cda3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Add origin and destination features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_destination_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_origin_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 2437 is out of bounds for axis 0 with size 2437"]}]},{"cell_type":"code","metadata":{"id":"NQdkNPSZBiwB"},"source":[""],"id":"NQdkNPSZBiwB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hL8ESdyX_XqH"},"source":[""],"id":"hL8ESdyX_XqH","execution_count":null,"outputs":[]}]}